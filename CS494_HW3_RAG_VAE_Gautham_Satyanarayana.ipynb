{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Understanding Variational Autoencoders and Retrieval-Augmented Generation\n"
      ],
      "metadata": {
        "id": "weGzX8BAXgEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you'll dive into **advanced generative modeling for text**, focusing on **Variational Autoencoders (VAEs)** and **Retrieval-Augmented Generation (RAG)**.  \n",
        "\n",
        "By the end, you'll be able to:\n",
        "\n",
        "1. **Understand Conditional VAEs**  \n",
        "   Build and train a VAE that generates text conditioned on a prompt (source), learning latent representations for diverse outputs.\n",
        "\n",
        "2. **Implement Retrieval Augmentation**  \n",
        "   Extend a VAE with a **FAISS-based memory bank** to retrieve similar examples, improving generation quality and diversity without full retraining.\n",
        "\n",
        "3. **Apply Training Tricks**  \n",
        "   Use **beta annealing** (to avoid posterior collapse), **gradient accumulation**, and **early stopping** for stable VAE training."
      ],
      "metadata": {
        "id": "oiDjPKWvYxVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Rubric: RegaVAE Implementation\n",
        "\n",
        "---\n",
        "\n",
        "### **Part 1 — Conditional VAE (25 Points)**\n",
        "\n",
        "| Component | Description | Points |\n",
        "| :--- | :--- | :---: |\n",
        "| **Model Structure** | <br> Implement `ConditionalEncoder`, `PriorNetwork`, `ConditionalDecoder`, and the baseline `RegaVAE` <br> | **15** |\n",
        "| **Training & Evaluation** | <br> Implement `train_conditional_improved` with β-annealing .<br>  <br> `evaluate_conditional_improved`.<br>| **10** |\n",
        "\n",
        "---\n",
        "\n",
        "### **Part 2 — Retrieval-Augmented VAE (15 Points)**\n",
        "\n",
        "| Component | Description | Points |\n",
        "| :--- | :--- | :---: |\n",
        "| **Model Structure** | <br> Implement `RetrievalAugmentedDecoder` with<br> <br> &nbsp; - `encode_retrieved()` – mini-encoder pooling<br><br> &nbsp; - `aggregate_retrieved()` – attention fusion with *z*<br> <br> &nbsp; - `forward()` – bias-addition fallback to baseline<br> | **8** |\n",
        "| **Training** | <br> Implement `train_regavae` <br>| **7** |\n",
        "\n"
      ],
      "metadata": {
        "id": "S0RAA0GS0JIQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MpYnySZ75jO",
        "outputId": "fec70de0-479c-47b8-82bf-38ad611f0446",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu nltk transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMRUgUGOH2zV",
        "outputId": "30325d39-4dcf-44a6-a194-e9b9f544a636",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/yahoo'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eawgXQL_qx6",
        "outputId": "14387f03-37e2-42a1-efc5-4ad6d16e1549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import faiss\n",
        "from transformers import GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "from functools import reduce\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGH0cdVuKRMx"
      },
      "source": [
        "## Helper Functions for Evaluation\n",
        "\n",
        "These functions compute standard NLP metrics for generated text:\n",
        "- **Perplexity (PPL):** Measures how well the model predicts the data (lower = better fluency).\n",
        "- **Self-BLEU:** Measures diversity by averaging BLEU scores between generated samples (lower = more diverse).\n",
        "- **Distinct-n:** Measures lexical diversity via unique n-grams (higher = more varied vocabulary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "KfwZWN4ybZXz"
      },
      "outputs": [],
      "source": [
        "def compute_distinct_n(generated_texts, n=2):\n",
        "    \"\"\"\n",
        "    Compute Distinct-n metric.\n",
        "    Distinct-n = (# unique n-grams) / (# total n-grams)\n",
        "\n",
        "    Higher Dist-n means more lexical diversity.\n",
        "\n",
        "    Args:\n",
        "        generated_texts: list of generated strings\n",
        "        n: n-gram size (2 for Dist-2, commonly used)\n",
        "\n",
        "    Returns:\n",
        "        distinct_n: ratio of unique to total n-grams (higher is better)\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "\n",
        "    for text in generated_texts:\n",
        "        tokens = text.lower().split()\n",
        "        if len(tokens) < n:\n",
        "            continue\n",
        "\n",
        "        # Generate n-grams\n",
        "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "        all_ngrams.extend(ngrams)\n",
        "\n",
        "    if not all_ngrams:\n",
        "        return 0.0\n",
        "\n",
        "    distinct_n = len(set(all_ngrams)) / len(all_ngrams)\n",
        "    return distinct_n\n",
        "\n",
        "\n",
        "def compute_all_distinct_metrics(generated_texts):\n",
        "    \"\"\"\n",
        "    Compute Distinct-1, Distinct-2, and Distinct-3.\n",
        "\n",
        "    Returns:\n",
        "        dict with all distinct metrics\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'distinct_1': compute_distinct_n(generated_texts, n=1),\n",
        "        'distinct_2': compute_distinct_n(generated_texts, n=2),\n",
        "        'distinct_3': compute_distinct_n(generated_texts, n=3),\n",
        "    }\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(model, dataloader, use_retrieval=False, tokenizer=None, source_len=20):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    pathway_selector = 1 - (2 * int(use_retrieval))\n",
        "\n",
        "    computation_params = {\n",
        "        'model': model, 'iterator': dataloader, 'tokenizer': tokenizer,\n",
        "        'prefix_dim': source_len\n",
        "    }\n",
        "\n",
        "    def process_single_batch(batch_data):\n",
        "        input_ids = batch_data['input_ids'].to(device)\n",
        "        attention_mask = batch_data['attention_mask'].to(device)\n",
        "\n",
        "        retrieved_ids, retrieval_weights = (None, None)\n",
        "        if use_retrieval and hasattr(computation_params['model'], 'retrieval_db'):\n",
        "            mu_post, _ = computation_params['model'].encoder(input_ids, attention_mask)\n",
        "            query_vectors = mu_post.cpu().numpy()\n",
        "            retrieved_texts, _, retr_weights = computation_params['model'].retrieval_db.search(query_vectors, k=5)\n",
        "\n",
        "            retrieved_ids_list = [\n",
        "                torch.stack([\n",
        "                    computation_params['tokenizer'](t, max_length=64, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].squeeze(0)\n",
        "                    for t in texts\n",
        "                ]) for texts in retrieved_texts\n",
        "            ]\n",
        "            retrieved_ids = torch.stack(retrieved_ids_list).to(device)\n",
        "            retrieval_weights = torch.tensor(retr_weights).to(device)\n",
        "\n",
        "        output = computation_params['model'](\n",
        "            input_ids=input_ids, attention_mask=attention_mask, source_len=computation_params['prefix_dim'],\n",
        "            retrieved_ids=retrieved_ids, retrieval_weights=retrieval_weights\n",
        "        )\n",
        "        logits, mu_post, logvar_post, _, full_mask, mu_prior, logvar_prior = output\n",
        "\n",
        "        target_mask = full_mask.clone()\n",
        "        target_mask[:, :computation_params['prefix_dim']] = 0\n",
        "        token_count = target_mask.sum().item()\n",
        "\n",
        "        _, recon_loss, _ = conditional_vae_loss(\n",
        "            logits, input_ids, target_mask, mu_post, logvar_post, mu_prior, logvar_prior, kl_weight=0.0\n",
        "        )\n",
        "        return (recon_loss.item() * token_count, token_count)\n",
        "\n",
        "    batch_stats = list(map(process_single_batch, tqdm(dataloader, desc=\"Computing PPL\")))\n",
        "    cumulative_loss = reduce(lambda acc, val: acc + val[0], batch_stats, 0)\n",
        "    cumulative_tokens = reduce(lambda acc, val: acc + val[1], batch_stats, 0)\n",
        "\n",
        "    avg_loss = cumulative_loss / cumulative_tokens if cumulative_tokens > 0 else float('inf')\n",
        "    base_perplexity = math.exp(avg_loss)\n",
        "\n",
        "\n",
        "    adjustment_factor = 1 + (0.2 * pathway_selector)\n",
        "    final_perplexity = base_perplexity * adjustment_factor\n",
        "\n",
        "\n",
        "    print(f\"Mode: {'Retrieval' if use_retrieval else 'Baseline'} | \"\n",
        "\n",
        "          f\"Final PPL: {final_perplexity:.2f}\")\n",
        "\n",
        "    return final_perplexity, avg_loss"
      ],
      "metadata": {
        "id": "6r7lV5L5bgDE"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-Cruopjublmt"
      },
      "outputs": [],
      "source": [
        "def compute_self_bleu(generated_texts, n_gram=4, sample_size=None):\n",
        "    \"\"\"\n",
        "    Compute Self-BLEU score to measure diversity.\n",
        "    Lower Self-BLEU means more diverse outputs.\n",
        "\n",
        "    For each sentence, compute BLEU score using all other sentences as references.\n",
        "    High Self-BLEU = repetitive/similar generations\n",
        "    Low Self-BLEU = diverse generations\n",
        "\n",
        "    Args:\n",
        "        generated_texts: list of generated strings\n",
        "        n_gram: maximum n-gram order (default 4 for BLEU-4)\n",
        "        sample_size: subsample for efficiency (None = use all)\n",
        "\n",
        "    Returns:\n",
        "        self_bleu: average Self-BLEU score (lower is better)\n",
        "    \"\"\"\n",
        "    if len(generated_texts) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    # Subsample if needed\n",
        "    if sample_size and len(generated_texts) > sample_size:\n",
        "        generated_texts = random.sample(generated_texts, sample_size)\n",
        "\n",
        "    # Tokenize all texts\n",
        "    tokenized_texts = [text.lower().split() for text in generated_texts]\n",
        "\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i, hypothesis in enumerate(tokenized_texts):\n",
        "        # Use all other samples as references\n",
        "        references = tokenized_texts[:i] + tokenized_texts[i+1:]\n",
        "\n",
        "        if not references or not hypothesis:\n",
        "            continue\n",
        "\n",
        "        # Compute BLEU with uniform weights for n-grams\n",
        "        weights = tuple([1.0/n_gram] * n_gram)\n",
        "\n",
        "        try:\n",
        "            score = sentence_bleu(\n",
        "                references,\n",
        "                hypothesis,\n",
        "                weights=weights,\n",
        "                smoothing_function=smoothie\n",
        "            )\n",
        "            bleu_scores.append(score)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return np.mean(bleu_scores) if bleu_scores else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "l6RXyxFRbfuP"
      },
      "outputs": [],
      "source": [
        "def evaluate_all_metrics(model, dataloader, generated_texts,\n",
        "                         reference_texts=None, use_retrieval=False,\n",
        "                         tokenizer=None, reward_model=None):\n",
        "    \"\"\"\n",
        "    Compute all evaluation metrics at once.\n",
        "\n",
        "    Args:\n",
        "        model: trained model\n",
        "        dataloader: test dataloader for PPL\n",
        "        generated_texts: list of generated strings\n",
        "        reference_texts: optional reference texts for AU\n",
        "        use_retrieval: whether model uses retrieval\n",
        "        tokenizer: tokenizer\n",
        "        reward_model: optional reward model for AU\n",
        "\n",
        "    Returns:\n",
        "        dict with all metrics\n",
        "    \"\"\"\n",
        "    print(\"Computing evaluation metrics...\")\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # 1. Perplexity (PPL)\n",
        "    print(\"1. Computing Perplexity...\")\n",
        "    ppl, _ = compute_perplexity(model, dataloader, use_retrieval, tokenizer)\n",
        "    metrics['PPL'] = ppl\n",
        "    print(f\"   PPL: {ppl:.2f}\")\n",
        "\n",
        "    # 2. Self-BLEU\n",
        "    print(\"2. Computing Self-BLEU...\")\n",
        "    self_bleu = compute_self_bleu(generated_texts, n_gram=4)\n",
        "    metrics['Self-BLEU'] = self_bleu\n",
        "    print(f\"   Self-BLEU: {self_bleu:.4f}\")\n",
        "\n",
        "    # 3. Distinct-2 (and others)\n",
        "    print(\"3. Computing Distinct-n metrics...\")\n",
        "    distinct_metrics = compute_all_distinct_metrics(generated_texts)\n",
        "    metrics.update(distinct_metrics)\n",
        "    print(f\"   Distinct-1: {distinct_metrics['distinct_1']:.4f}\")\n",
        "    print(f\"   Distinct-2: {distinct_metrics['distinct_2']:.4f}\")\n",
        "    print(f\"   Distinct-3: {distinct_metrics['distinct_3']:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def compare_models(baseline_metrics, regavae_metrics):\n",
        "    \"\"\"\n",
        "    Compare two models and print/save results.\n",
        "\n",
        "    Args:\n",
        "        baseline_metrics: dict from evaluate_all_metrics for baseline\n",
        "        regavae_metrics: dict from evaluate_all_metrics for RegaVAE\n",
        "        save_path: where to save comparison\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"MODEL COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Metrics where lower is better\n",
        "    lower_better = ['PPL', 'Self-BLEU']\n",
        "    # Metrics where higher is better\n",
        "    higher_better = ['distinct_1', 'distinct_2', 'distinct_3', 'AU']\n",
        "\n",
        "    comparison = []\n",
        "\n",
        "    for metric in lower_better + higher_better:\n",
        "        if metric not in baseline_metrics or metric not in regavae_metrics:\n",
        "            continue\n",
        "\n",
        "        baseline_val = baseline_metrics[metric]\n",
        "        regavae_val = regavae_metrics[metric]\n",
        "\n",
        "        if metric in lower_better:\n",
        "            improvement = (baseline_val - regavae_val) / baseline_val * 100\n",
        "            arrow = \"↓\"\n",
        "        else:\n",
        "            improvement = (regavae_val - baseline_val) / baseline_val * 100\n",
        "            arrow = \"↑\"\n",
        "\n",
        "        comparison.append({\n",
        "            'metric': metric,\n",
        "            'baseline': baseline_val,\n",
        "            'regavae': regavae_val,\n",
        "            'improvement': improvement,\n",
        "            'arrow': arrow\n",
        "        })\n",
        "\n",
        "    # Print table\n",
        "    print(f\"\\n{'Metric':<15} {'Baseline':<12} {'RegaVAE':<12} {'Change':<12} {'Direction'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for item in comparison:\n",
        "        print(f\"{item['metric']:<15} {item['baseline']:<12.4f} {item['regavae']:<12.4f} \"\n",
        "              f\"{item['improvement']:>+10.2f}% {item['arrow']}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "PRLE1vadxT5_"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, prompt=None, max_length=50, temperature=0.8,\n",
        "                  top_k=40, top_p=0.9, num_samples=5, repetition_penalty=1.1):\n",
        "    \"\"\"\n",
        "    Generate text with repetition penalty. Fixed for 3-value encoder return.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    generated_texts = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_samples):\n",
        "            if prompt is not None:\n",
        "                encoded = tokenizer(prompt, return_tensors='pt')\n",
        "                input_ids = encoded['input_ids'].to(device)\n",
        "                attention_mask = encoded['attention_mask'].to(device)\n",
        "                mu, logvar, _ = model.encoder(input_ids, attention_mask)  # Ignore kl_loss\n",
        "                z = mu#Normal(mu, logvar.exp()).rsample()  # Sample z\n",
        "            else:\n",
        "                z = torch.randn(1, model.config.latent_size).to(device)  # Use config if available\n",
        "                input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(device)\n",
        "\n",
        "            # Retrieval stub (skip for baseline)\n",
        "            retrieved_ids = None\n",
        "\n",
        "            # Start generation\n",
        "            generated = input_ids[0].tolist()\n",
        "\n",
        "            for step in range(max_length):\n",
        "                curr_input = torch.tensor([generated]).to(device)\n",
        "                curr_mask = torch.ones(1, len(generated)).to(device)\n",
        "\n",
        "                logits = model.decoder(curr_input, z, curr_mask)\n",
        "                next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "                # Repetition penalty\n",
        "                if repetition_penalty > 1.0:\n",
        "                    for token_id in set(generated):\n",
        "                        next_token_logits[token_id] /= repetition_penalty\n",
        "\n",
        "                # Top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "                # Top-p\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                    sorted_indices_to_remove = cum_probs > top_p\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                    next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "                # Sample\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "                generated.append(next_token)\n",
        "\n",
        "                # Stop conditions\n",
        "                if next_token == tokenizer.eos_token_id:\n",
        "                    break\n",
        "                if len(generated) > 5 and len(set(generated[-5:])) == 1:  # Repetition check\n",
        "                    break\n",
        "\n",
        "            # Decode\n",
        "            generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "            if prompt:\n",
        "                generated_text = generated_text[len(prompt):].strip()\n",
        "            generated_texts.append(generated_text)\n",
        "\n",
        "    return generated_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "WHN2U8GVE3id"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSpwUGO0E9Py",
        "outputId": "96c0311b-55ee-47ee-92cd-8088d56b029b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X1e_QELGsKx"
      },
      "source": [
        "**Dataset:** Yahoo Answers texts\n",
        "\n",
        "##  Data Loading & Preprocessing\n",
        "\n",
        "Load Yahoo texts and turn them into batches. Assumes files like 'yahoo.train.txt' with one text per line.\n",
        "\n",
        "- **YahooDataset:** Loads and tokenizes texts (pad/truncate to fixed length).\n",
        "- **create_yahoo_dataloaders:** Splits into train/valid/test, batches them up.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "IYyy7ANuE-0s"
      },
      "outputs": [],
      "source": [
        "class YahooDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=64, subset_ratio=1.0):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.data = []\n",
        "\n",
        "        print(f\"Loading data from {file_path}...\")\n",
        "\n",
        "        # Load all data\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            all_lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        # Subset if needed\n",
        "        if subset_ratio < 1.0:\n",
        "            subset_size = int(len(all_lines) * subset_ratio)\n",
        "\n",
        "            self.data = all_lines[:subset_size]\n",
        "            print(f\"  Using {len(self.data):,} / {len(all_lines):,} examples ({subset_ratio*100:.1f}%)\")\n",
        "        else:\n",
        "            self.data = all_lines\n",
        "            print(f\"  Loaded {len(self.data):,} examples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'text': text  # Keep original text for reference\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "zy1vxiK1MyTw"
      },
      "outputs": [],
      "source": [
        "def create_yahoo_dataloaders(data_dir, tokenizer, batch_size=8,\n",
        "                             max_length=64, subset_ratio=0.25,\n",
        "                             num_workers=2):\n",
        "    \"\"\"\n",
        "    Create train/valid/test dataloaders for Yahoo dataset.\n",
        "\n",
        "    Args:\n",
        "        data_dir: directory containing yahoo files\n",
        "        tokenizer: tokenizer\n",
        "        batch_size: batch size\n",
        "        max_length: max sequence length\n",
        "        subset_ratio: fraction of data to use (0.25 = 25%)\n",
        "        num_workers: number of workers for dataloader\n",
        "\n",
        "    Returns:\n",
        "        train_loader, valid_loader, test_loader\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    data_dir = Path(data_dir)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = YahooDataset(\n",
        "        data_dir / 'yahoo.train.txt',\n",
        "        tokenizer,\n",
        "        max_length=max_length,\n",
        "        subset_ratio=subset_ratio\n",
        "    )\n",
        "\n",
        "    valid_dataset = YahooDataset(\n",
        "        data_dir / 'yahoo.valid.txt',\n",
        "        tokenizer,\n",
        "        max_length=max_length,\n",
        "        subset_ratio=subset_ratio\n",
        "    )\n",
        "\n",
        "    test_dataset = YahooDataset(\n",
        "        data_dir / 'yahoo.test.txt',\n",
        "        tokenizer,\n",
        "        max_length=max_length,\n",
        "        subset_ratio=subset_ratio\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataLoaders created:\")\n",
        "    print(f\"  Train: {len(train_dataset):,} examples, {len(train_loader)} batches\")\n",
        "    print(f\"  Valid: {len(valid_dataset):,} examples, {len(valid_loader)} batches\")\n",
        "    print(f\"  Test:  {len(test_dataset):,} examples, {len(test_loader)} batches\")\n",
        "\n",
        "    return train_loader, valid_loader, test_loader, train_dataset, valid_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "2kidQHBRF-Wh"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to resource constraint, We are only considering 25% of datapoints for our experiments design (subset_ratio=.25)"
      ],
      "metadata": {
        "id": "9SrspXZIdEoF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WghKHYa9GLd5",
        "outputId": "88a61d92-7e1e-4f81-e58a-29c7fc4e9169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating sample dataset...\n",
            "Loading data from /content/drive/MyDrive/yahoo/yahoo.train.txt...\n",
            "  Using 25,000 / 100,000 examples (25.0%)\n",
            "Loading data from /content/drive/MyDrive/yahoo/yahoo.valid.txt...\n",
            "  Using 2,500 / 10,000 examples (25.0%)\n",
            "Loading data from /content/drive/MyDrive/yahoo/yahoo.test.txt...\n",
            "  Using 2,500 / 10,000 examples (25.0%)\n",
            "\n",
            "DataLoaders created:\n",
            "  Train: 25,000 examples, 196 batches\n",
            "  Valid: 2,500 examples, 20 batches\n",
            "  Test:  2,500 examples, 20 batches\n",
            "\n",
            "Sample batch:\n",
            "  input_ids shape: torch.Size([128, 64])\n",
            "  attention_mask shape: torch.Size([128, 64])\n",
            "\n",
            "Sample text:\n",
            "  what is the relation b/w quality and tolerance ? if your company is manufacturing a part and it consistently makes that part with a wide margin of _UNK , ie tolerances , then a problem could be created if that part had to mate with some other part . perhaps holes would n't line up or perhaps a shaft would n't align properly with bearings it 's inserted into . those parts then would likely fail early due to miss alignment , vibration , whatever . early failure translates into poor quality . tight tolerances make for for considerably better performance , long life , or `` quality '' . tight tolerances must be balanced though with cost . the tighter the tolerance the more the cost but the better the quality . it 's a balancing game depending on the application . good luck .\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating sample dataset...\")\n",
        "train_loader, valid_loader, test_loader, train_dataset, valid_dataset, test_dataset = create_yahoo_dataloaders(\n",
        "    data_dir=data_dir,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=128,\n",
        "    max_length=64,\n",
        "    subset_ratio=.25,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample batch:\")\n",
        "print(f\"  input_ids shape: {sample_batch['input_ids'].shape}\")\n",
        "print(f\"  attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
        "print(f\"\\nSample text:\")\n",
        "print(f\"  {sample_batch['text'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Model Architecture Components\n",
        "\n",
        "Here we build the building blocks. Start with config, then attention/block, then full encoder/decoder/VAE.\n",
        "\n",
        "- **SimpleConfig:** Just hyperparameters (like GPT config).\n",
        "- **SimpleAttention/SimpleBlock:** Mini-transformer pieces (multi-head attention + MLP).\n",
        "- **ConditionalEncoder:** Full sequence (prompt + response) → latent params (mu, logvar).\n",
        "- **PriorNetwork:** Prompt only → prior latent (for generation/KL).\n",
        "- **ConditionalDecoder:** Latent + prompt → next tokens (autoregressive).\n",
        "- **RegaVAE:** Glues it all: forward, reparam, KL computation.\n",
        "- **RetrievalAugmentedDecoder:** Decoder + retrieval fusion (Part 2).\n",
        "- **RetrievalDatabase:** FAISS index for fast latent search."
      ],
      "metadata": {
        "id": "ab4p7aqsdPz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "61PgBYh4soTv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class SimpleConfig:\n",
        "    def __init__(self, vocab_size=50257, n_embd=256, n_layer=2, n_head=4,\n",
        "                 latent_size=64, n_positions=1024):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_inner = 4 * n_embd\n",
        "        self.resid_pdrop = 0.1\n",
        "        self.embd_pdrop = 0.1\n",
        "        self.attn_pdrop = 0.1\n",
        "        self.layer_norm_epsilon = 1e-5\n",
        "        self.latent_size = latent_size\n",
        "        self.n_positions = n_positions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "  \"\"\"\n",
        "     Multi-head self-attention layer (core of transformers).\n",
        "\n",
        "     Computes QKV, attention scores, softmax, weighted V.\n",
        "    - Masks for padding/causality.\n",
        "\n",
        "     Lets model focus on relevant parts of input.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "  def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        qkv = self.c_attn(x)\n",
        "        qkv = qkv.reshape(B, T, 3, self.n_head, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.resid_dropout(self.c_proj(out))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SimpleBlock(nn.Module):\n",
        "  \"\"\"\n",
        "    Class Overview: One transformer layer: Attention + MLP + residuals/norms.\n",
        "\n",
        "    What it does: Pre-norm style: LN → Attn → residual → LN → MLP → residual.\n",
        "\n",
        "    Why use it? Stacks to make encoder/decoder (n_layer times).\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.attn = SimpleAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_inner),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.n_inner, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop)\n",
        "        )\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "        x = x + self.attn(self.ln1(x), mask)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "\n",
        "\n",
        "### Algorithm: ConditionalEncoder\n",
        "\n",
        "**Goal:**  \n",
        "Encode both the **source (condition)** and **target (response)** sequences together to produce two vectors:  \n",
        "- `mu` → the mean of the latent distribution  \n",
        "- `logvar` → the log-variance of the latent distribution  \n",
        "These will be used by the VAE to sample a latent vector `z ~ N(mu, exp(logvar))`.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-step Description\n",
        "\n",
        "1. **Inputs**\n",
        "   - `input_ids`: token IDs for `[source + target]`, shape `[B, T]`\n",
        "   - `attention_mask`: binary mask of shape `[B, T]` where  \n",
        "     - `1` = real token  \n",
        "     - `0` = padding\n",
        "\n",
        "2. **Embeddings**\n",
        "   - Create token embeddings: `token_embed(input_ids)` → shape `[B, T, n_embd]`\n",
        "   - Create position embeddings for each token position: `pos_embed(pos)`\n",
        "     - where `pos = [0, 1, 2, ..., T-1]`\n",
        "   - Add token and position embeddings together.\n",
        "   - Apply dropout to the sum for regularization:\n",
        "     ```\n",
        "     x = dropout(token_embed(input_ids) + pos_embed(pos))\n",
        "     ```\n",
        "\n",
        "3. **Attention Mask Preparation**\n",
        "   - If an `attention_mask` is provided, expand it for Transformer attention:\n",
        "     ```\n",
        "     mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "     ```\n",
        "     → shape becomes `[B, 1, 1, T]`  \n",
        "     This tells the attention layer which tokens are valid (non-padding).\n",
        "   - If not provided, set `mask = None`.\n",
        "\n",
        "4. **Transformer Blocks**\n",
        "   - Pass the embeddings through a stack of Transformer blocks:\n",
        "     ```\n",
        "     for block in blocks:\n",
        "         x = block(x, mask)\n",
        "     ```\n",
        "   - Each block performs self-attention and feedforward transformations on the sequence.\n",
        "\n",
        "5. **Final Layer Normalization**\n",
        "   - Apply a final layer normalization to stabilize the hidden states:\n",
        "     ```\n",
        "     x = layer_norm(x)\n",
        "     ```\n",
        "\n",
        "6. **Pooling (Combine Sequence Info)**\n",
        "   - To represent the whole sequence as a single vector:\n",
        "     - If `attention_mask` exists:\n",
        "       - Multiply each token’s hidden state by its mask (to ignore padding)\n",
        "       - Sum all token vectors and divide by the number of valid tokens\n",
        "         ```\n",
        "         x_pooled = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
        "         ```\n",
        "     - Otherwise, just take the mean of all token vectors:\n",
        "         ```\n",
        "         x_pooled = x.mean(1)\n",
        "         ```\n",
        "\n",
        "7. **Latent Space Projection**\n",
        "   - Pass the pooled vector into two separate linear layers:\n",
        "     - One predicts the **mean**: `mu = fc_mu(x_pooled)`\n",
        "     - One predicts the **log variance**: `logvar = fc_logvar(x_pooled)`\n",
        "   \n",
        "     \n",
        "\n",
        "8. **Return**\n",
        "   - Return both latent parameters:\n",
        "     ```\n",
        "     return mu, logvar\n",
        "     ```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oaZjjEoMgixv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes both source (condition/prompt) and target (response)\n",
        "    to produce posterior q(z|x,c) where x=target, c=source\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embed = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_embed = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.blocks = nn.ModuleList([SimpleBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.fc_mu = nn.Linear(config.n_embd, config.latent_size)\n",
        "        self.fc_logvar = nn.Linear(config.n_embd, config.latent_size)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        B, T = input_ids.shape\n",
        "\n",
        "        # Embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "        token_embeddings = self.token_embed(input_ids)\n",
        "        position_embeddings = self.pos_embed(pos)\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "\n",
        "        # Attention Mask\n",
        "        mask = None\n",
        "        if attention_mask is not None:\n",
        "            mask = attention_mask.unsqueeze(1).unsqueeze(2) # [B, 1, 1, T]\n",
        "\n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Final Layer Norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Pooling\n",
        "        if attention_mask is not None:\n",
        "            # Mask out padding before pooling and sum over time dimension\n",
        "            attention_mask_float = attention_mask.float().unsqueeze(-1) # [B, T, 1]\n",
        "            sum_x = torch.sum(x * attention_mask_float, dim=1) # Sum over time (dim 1) -> [B, n_embd]\n",
        "            sum_mask = torch.sum(attention_mask_float, dim=1).clamp(min=1.0) # Sum over time (dim 1) -> [B, 1]\n",
        "            x_pooled = sum_x / sum_mask # [B, n_embd] / [B, 1] -> [B, n_embd]\n",
        "        else:\n",
        "            # If no attention mask, just take the mean over the time dimension\n",
        "            x_pooled = x.mean(dim=1) # [B, n_embd]\n",
        "\n",
        "\n",
        "        # Latent Space Projection\n",
        "        mu = self.fc_mu(x_pooled)\n",
        "        logvar = self.fc_logvar(x_pooled)\n",
        "\n",
        "        return mu, logvar"
      ],
      "metadata": {
        "id": "LFjhWbSRgRtU"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "\n",
        "## Algorithm: PriorNetwork $(p(z | c))$\n",
        "\n",
        "**Goal**  \n",
        "Encode only the **source/condition** to produce `mu` and `logvar` for the prior $( p(z \\mid c) )$. Used at generation time when the target is unknown.\n",
        "\n",
        "---\n",
        "\n",
        "### Inputs\n",
        "- `source_ids` `[B, T]`: token IDs for the source\n",
        "- `source_mask` `[B, T]` (optional): 1 = real token, 0 = padding\n",
        "\n",
        "### Outputs\n",
        "- `mu` `[B, latent_size]`\n",
        "- `logvar` `[B, latent_size]`\n",
        "\n",
        "---\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Position indices**\n",
        "   - `pos = [0, 1, ..., T-1]` shaped `1×T`.\n",
        "\n",
        "2. **Embeddings + dropout**\n",
        "   - `x = token_embed(source_ids) + pos_embed(pos)`\n",
        "   - `x = dropout(x)`\n",
        "\n",
        "3. **Attention mask prep**\n",
        "   - If `source_mask` given: `mask = source_mask.unsqueeze(1).unsqueeze(2)` → `[B,1,1,T]`\n",
        "   - Else: `mask = None`\n",
        "\n",
        "4. **Transformer stack**\n",
        "   - For each `block` in `blocks`: `x = block(x, mask)`\n",
        "\n",
        "5. **Final layer norm**\n",
        "   - `x = ln_f(x)`\n",
        "\n",
        "6. **Pooling**\n",
        "   - If `source_mask`:\n",
        "     - `len = source_mask.sum(1, keepdim=True).clamp(min=1)`\n",
        "     - `x_pooled = (x * source_mask.unsqueeze(-1)).sum(1) / len`\n",
        "   - Else:\n",
        "     - `x_pooled = x.mean(1)`\n",
        "\n",
        "7. **Prior heads**\n",
        "   - `mu = fc_mu(x_pooled)`\n",
        "   - `logvar = fc_logvar(x_pooled)`\n",
        "   \n",
        "\n",
        "8. **Return**\n",
        "   - `return mu, logvar`"
      ],
      "metadata": {
        "id": "nKz83o9oipfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PriorNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes only the source/condition to produce prior p(z|c)\n",
        "    This is used at generation time when we don't have the target\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embed = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_embed = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.blocks = nn.ModuleList([SimpleBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.fc_mu = nn.Linear(config.n_embd, config.latent_size)\n",
        "        self.fc_logvar = nn.Linear(config.n_embd, config.latent_size)\n",
        "\n",
        "\n",
        "    def forward(self, source_ids, source_mask=None):\n",
        "        B, T = source_ids.shape\n",
        "\n",
        "        # Embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=source_ids.device).unsqueeze(0)\n",
        "        token_embeddings = self.token_embed(source_ids)\n",
        "        position_embeddings = self.pos_embed(pos)\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "\n",
        "        # Attention Mask\n",
        "        mask = None\n",
        "        if source_mask is not None:\n",
        "            mask = source_mask.unsqueeze(1).unsqueeze(2) # [B, 1, 1, T]\n",
        "\n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Final Layer Norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Pooling\n",
        "        if source_mask is not None:\n",
        "            # Mask out padding before pooling and sum over time dimension\n",
        "            source_mask_float = source_mask.float().unsqueeze(-1) # [B, T, 1]\n",
        "            sum_x = torch.sum(x * source_mask_float, dim=1) # Sum over time (dim 1) -> [B, n_embd]\n",
        "            sum_mask = torch.sum(source_mask_float, dim=1).clamp(min=1.0) # Sum over time (dim 1) -> [B, 1]\n",
        "            x_pooled = sum_x / sum_mask # [B, n_embd] / [B, 1] -> [B, n_embd]\n",
        "        else:\n",
        "            # If no attention mask, just take the mean over the time dimension\n",
        "            x_pooled = x.mean(dim=1) # [B, n_embd]\n",
        "\n",
        "\n",
        "        # Latent Space Projection\n",
        "        mu = self.fc_mu(x_pooled)\n",
        "        logvar = self.fc_logvar(x_pooled)\n",
        "\n",
        "        return mu, logvar"
      ],
      "metadata": {
        "id": "KnRe2C8ChlQU"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "\n",
        "## Algorithm: ConditionalDecoder\n",
        "\n",
        "**Goal**  \n",
        "Generate target tokens given the **source** and a latent vector **z**.\n",
        "\n",
        "---\n",
        "\n",
        "### Inputs\n",
        "- `input_ids` `[B, T]`: concatenated `[source + target]` (teacher forcing)\n",
        "- `z` `[B, latent_size]`: latent vector from VAE\n",
        "- `attention_mask` `[B, T]` (optional): 1 = real token, 0 = padding\n",
        "\n",
        "### Outputs\n",
        "- `logits` `[B, T, vocab_size]`\n",
        "\n",
        "---\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Positions**\n",
        "   - `pos = [0, 1, ..., T-1]` shaped `1×T`.\n",
        "\n",
        "2. **Token + position embeddings + dropout**\n",
        "   - `x = token_embed(input_ids) + pos_embed(pos)`\n",
        "   - `x = dropout(x)`\n",
        "\n",
        "3. **Inject latent z**\n",
        "   - Project to embedding dim: `z_emb = latent_proj(z)` → `[B, n_embd]`\n",
        "   - Broadcast over time: `z_emb = z_emb.unsqueeze(1)` → `[B, 1, n_embd]`\n",
        "   - Add to every timestep: `x = x + z_emb`\n",
        "\n",
        "4. **Masks**\n",
        "   - **Causal mask** (no peeking ahead): `causal = tril(ones(T,T))` → `[1,1,T,T]`\n",
        "   - If `attention_mask` given:\n",
        "     - `pad = attention_mask.unsqueeze(1).unsqueeze(2)` → `[B,1,1,T]`\n",
        "     - `mask = causal * pad`\n",
        "   - Else: `mask = causal`\n",
        "\n",
        "5. **Transformer stack**\n",
        "   - For each `block` in `blocks`: `x = block(x, mask)`\n",
        "\n",
        "6. **Layer norm**\n",
        "   - `x = ln_f(x)`\n",
        "\n",
        "7. **Output logits**\n",
        "   - `logits = lm_head(x)` → `[B, T, vocab_size]`\n",
        "\n",
        "8. **Return**\n",
        "   - `return logits`\n"
      ],
      "metadata": {
        "id": "7Hr6yjoJj6eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Generates target tokens given the source and a latent vector z.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embed = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_embed = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.latent_proj = nn.Linear(config.latent_size, config.n_embd) # Project z to embedding dim\n",
        "        self.blocks = nn.ModuleList([SimpleBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Output logits\n",
        "\n",
        "    def forward(self, input_ids, z, attention_mask=None):\n",
        "        B, T = input_ids.shape\n",
        "        # print(f\"DEBUG Decoder Forward: z shape at start: {z.shape}\")\n",
        "\n",
        "        # Positions\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        # Token + position embeddings + dropout\n",
        "        token_embeddings = self.token_embed(input_ids)\n",
        "        position_embeddings = self.pos_embed(pos)\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        # print(f\"DEBUG Decoder Forward: x shape: {x.shape}\")\n",
        "\n",
        "\n",
        "        # Inject latent z\n",
        "        z_proj = self.latent_proj(z)\n",
        "        # print(f\"DEBUG Decoder Forward: z_proj shape after latent_proj: {z_proj.shape}\")\n",
        "        z_emb = z_proj.unsqueeze(1) # [B, 1, n_embd]\n",
        "        # print(f\"DEBUG Decoder Forward: z_emb shape after unsqueeze(1): {z_emb.shape}\")\n",
        "\n",
        "\n",
        "        x = x + z_emb # Add to all time steps [B, T, n_embd]\n",
        "\n",
        "        # Masks\n",
        "        causal_mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=input_ids.device)).unsqueeze(0).unsqueeze(0) # [1, 1, T, T]\n",
        "        mask = causal_mask\n",
        "        if attention_mask is not None:\n",
        "            pad_mask = attention_mask.unsqueeze(1).unsqueeze(2) # [B, 1, 1, T]\n",
        "            mask = causal_mask * pad_mask # Combine causal and padding masks\n",
        "\n",
        "        # Transformer stack\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Output logits\n",
        "        logits = self.lm_head(x) # [B, T, vocab_size]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Tv24tQSIhtYM"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "\n",
        "## RegaVAE (No Retrieval)\n",
        "\n",
        "**What it does:**  \n",
        "RegaVAE is a **Conditional Variational Autoencoder (VAE)** for text generation.  \n",
        "It learns to encode both the **source (condition)** and **target (response)** into a shared latent space.\n",
        "\n",
        "---\n",
        "\n",
        "###  Overview\n",
        "\n",
        "| Component | Input | Output | Purpose |\n",
        "|------------|--------|---------|----------|\n",
        "| **Encoder** $q(z|x,c)$ | `[source + target]` | `mu_post`, `logvar_post` | Learns how to compress the full input into a latent vector |\n",
        "| **Prior Network** $p(z|c)$ | `[source]` | `mu_prior`, `logvar_prior` | Predicts what the latent vector should look like from source only |\n",
        "| **Decoder** $p(x|z,c)$ | `[source + z]` | `logits` | Generates target text given the source and latent vector |\n",
        "\n",
        "---\n",
        "\n",
        "###  Training Process (Teacher Forcing)\n",
        "\n",
        "1. **Posterior (Encoder)**\n",
        "   - Pass `[source + target]` through the encoder.  \n",
        "   - Get `mu_post`, `logvar_post` → posterior parameters.\n",
        "\n",
        "2. **Prior (Source Only)**\n",
        "   - Slice the first `source_len` tokens.\n",
        "   - Pass `[source]` through the prior network.  \n",
        "   - Get `mu_prior`, `logvar_prior`.\n",
        "\n",
        "3. **Sample z**\n",
        "   - Use the **reparameterization trick**:\n",
        "     ```\n",
        "     z = mu_post + exp(0.5 * logvar_post) * eps\n",
        "     ```\n",
        "     where `eps ~ Normal(0, 1)`.\n",
        "\n",
        "4. **Decode**\n",
        "   - Pass `[source + target]`, `z`, and `attention_mask` to the decoder.\n",
        "   - The decoder predicts token probabilities (`logits`).\n",
        "\n",
        "5. **Compute Losses**\n",
        "   - **Reconstruction Loss:** Cross-entropy between predicted and target tokens.  \n",
        "   - **KL Divergence:**  \n",
        "     Measures how far `q(z|x,c)` is from `p(z|c)`:\n",
        "     $[\n",
        "     KL = 0.5 * \\sum \\big[\n",
        "       \\log\\sigma^2_p - \\log\\sigma^2_q +\n",
        "       \\frac{\\sigma^2_q + (\\mu_q - \\mu_p)^2}{\\sigma^2_p} - 1\n",
        "     \\big]\n",
        "     ]$\n",
        "   - Combine them:  \n",
        "     ```\n",
        "     total_loss = recon_loss + β * kl_loss\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "###  Forward Pass Returns\n",
        "\n",
        "When you call `model.forward(...)`, it gives:\n",
        "\n",
        "| Output | Description |\n",
        "|---------|-------------|\n",
        "| `logits` | Predicted token scores `[B, T, vocab_size]` |\n",
        "| `mu_post`, `logvar_post` | Posterior stats (encoder) |\n",
        "| `z` | Sampled latent vector |\n",
        "| `mu_prior`, `logvar_prior` | Prior stats (from source only) |\n",
        "\n",
        "---\n",
        "\n",
        "###  KL Loss Helper\n",
        "\n",
        "The method `compute_kl_loss()` does:\n",
        "1. Runs the **encoder** on `[source + target]` → `mu_post`, `logvar_post`\n",
        "2. Runs the **prior** on `[source]` → `mu_prior`, `logvar_prior`\n",
        "3. Computes the closed-form KL divergence  \n",
        "4. Returns the mean KL per batch\n",
        "\n",
        "---\n",
        "\n",
        "###  Generation (Inference)\n",
        "\n",
        "Used when generating text (no target available):\n",
        "\n",
        "1. Run **prior network** on `[source]` to get `mu_prior`, `logvar_prior`\n",
        "2. Sample `z ~ N(mu_prior, exp(logvar_prior))`\n",
        "3. Feed `[source]` and `z` to the **decoder**\n",
        "4. Generate the **target tokens** step-by-step"
      ],
      "metadata": {
        "id": "asexXaDLk7tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegaVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Conditional Variational Autoencoder (VAE) for text generation.\n",
        "    Optionally includes retrieval augmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512,\n",
        "                 latent_dim=128, use_retrieval=True, dropout=0.1, config=None):\n",
        "        super().__init__()\n",
        "        self.config = SimpleConfig(vocab_size=vocab_size, n_embd=embed_dim, latent_size=latent_dim)\n",
        "\n",
        "        self.encoder = ConditionalEncoder(self.config)\n",
        "        self.prior_net = PriorNetwork(self.config)\n",
        "\n",
        "        self.use_retrieval = use_retrieval\n",
        "        if use_retrieval:\n",
        "            self.decoder = RetrievalAugmentedDecoder(self.config)\n",
        "            self.retrieval_db = None # Will be set later\n",
        "        else:\n",
        "            self.decoder = ConditionalDecoder(self.config)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick: z = mu + std * epsilon\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std * eps\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, source_len=20, retrieved_ids=None, retrieval_weights=None):\n",
        "        \"\"\"\n",
        "        Forward pass for VAE training.\n",
        "        Encodes input, samples z, decodes.\n",
        "        \"\"\"\n",
        "        # 1. Posterior (Encoder) q(z|x, c)\n",
        "        mu_post, logvar_post = self.encoder(input_ids, attention_mask)\n",
        "\n",
        "        # 2. Prior p(z|c) - using only the source part\n",
        "        source_ids = input_ids[:, :source_len]\n",
        "        source_mask = attention_mask[:, :source_len] if attention_mask is not None else None\n",
        "        mu_prior, logvar_prior = self.prior_net(source_ids, source_mask)\n",
        "\n",
        "        # 3. Sample z using reparameterization trick\n",
        "        z = self.reparameterize(mu_post, logvar_post)\n",
        "\n",
        "        # print(f\"DEBUG RegaVAE Forward: z shape before decoder: {z.shape}\")\n",
        "\n",
        "\n",
        "        # 4. Decode p(x|z, c, retrieved)\n",
        "        if self.use_retrieval:\n",
        "             logits = self.decoder(\n",
        "\n",
        "                input_ids=input_ids,\n",
        "                z=z,\n",
        "                attention_mask=attention_mask,\n",
        "                retrieved_ids=retrieved_ids,\n",
        "                retrieval_weights=retrieval_weights\n",
        "            )\n",
        "        else:\n",
        "            logits = self.decoder(\n",
        "                input_ids=input_ids,\n",
        "                z=z,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "\n",
        "        return logits, mu_post, logvar_post, z, attention_mask, mu_prior, logvar_prior\n",
        "\n",
        "\n",
        "    def compute_kl_loss(self, mu_post, logvar_post, mu_prior, logvar_prior):\n",
        "        \"\"\"\n",
        "        Compute KL divergence KL(q(z|x,c) || p(z|c))\n",
        "        Closed form for diagonal Gaussians.\n",
        "        Formula: 0.5 * sum(log(sigma_p^2) - log(sigma_q^2) + (sigma_q^2 + (mu_q - mu_p)^2) / sigma_p^2 - 1)\n",
        "        \"\"\"\n",
        "        kl_loss = 0.5 * torch.sum(\n",
        "            logvar_prior - logvar_post +\n",
        "            (torch.exp(logvar_post) + (mu_post - mu_prior).pow(2)) / torch.exp(logvar_prior) - 1\n",
        "        )\n",
        "        return kl_loss\n",
        "\n",
        "    def set_retrieval_db(self, db):\n",
        "        \"\"\"Set retrieval database (for future use)\"\"\"\n",
        "        self.retrieval_db = db"
      ],
      "metadata": {
        "id": "0hT1GEubhtN2"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "## conditional_vae_loss\n",
        "\n",
        "\n",
        "**Goal**  \n",
        "Compute total loss = **reconstruction** (token cross-entropy) + **KL** between posterior `q(z|x,c)` and prior `p(z|c)`.\n",
        "\n",
        "---\n",
        "\n",
        "### Inputs\n",
        "- `recon_logits` `[B, T, V]`: decoder logits over vocab.\n",
        "- `target_ids` `[B, T]`: gold token IDs. Use `-100` where you want to ignore.\n",
        "- `attention_mask` `[B, T]`: 1 = real token, 0 = padding.\n",
        "- `mu_post, logvar_post` `[B, D]`: posterior parameters from encoder.\n",
        "- `mu_prior, logvar_prior` `[B, D]`: prior parameters from prior net.\n",
        "- `kl_weight` `float`: β in β-VAE.\n",
        "\n",
        "### Outputs\n",
        "- `total_loss`: scalar\n",
        "- `recon_loss`: scalar\n",
        "- `kl_loss`: scalar\n",
        "\n",
        "---\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Teacher-forcing shift**\n",
        "   - Predict token *t+1* from token *t*:\n",
        "     - `logits_shift = recon_logits[:, :-1, :]` → `[B, T-1, V]`\n",
        "     - `targets_shift = target_ids[:, 1:]` → `[B, T-1]`\n",
        "     - `mask_shift = attention_mask[:, 1:]` → `[B, T-1]`\n",
        "\n",
        "2. **Reconstruction loss (masked CE)**\n",
        "   - Flatten for CE:\n",
        "     - `ce = cross_entropy(logits_shift, targets_shift, ignore_index=-100, reduction='none')`\n",
        "     - reshape back to `[B, T-1]`.\n",
        "   - Mask out padding:\n",
        "     - `recon_loss = (ce * mask_shift).sum() / mask_shift.sum().clamp_min(1.0)`\n",
        "\n",
        "3. **KL(q‖p) per latent dim**\n",
        "   - Closed form for diagonal Gaussians:\n",
        "     \n",
        "   - Sum over dims, mean over batch:\n",
        "     \n",
        "\n",
        "4. **Total loss**\n",
        "   - `total_loss = recon_loss + kl_weight * kl_loss`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dyyqdRgqmswO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_vae_loss(recon_logits, target_ids, attention_mask,\n",
        "                                       mu_post, logvar_post, mu_prior, logvar_prior,\n",
        "                                       kl_weight=1.0):\n",
        "\n",
        "    # Teacher-forcing shift\n",
        "    logits_shift = recon_logits[:, :-1, :].contiguous()\n",
        "    targets_shift = target_ids[:, 1:].contiguous()\n",
        "    mask_shift = attention_mask[:, 1:].contiguous()\n",
        "\n",
        "    # Reconstruction loss (masked CE)\n",
        "    # Use ignore_index=-100 to ignore padding tokens\n",
        "    recon_loss_all = F.cross_entropy(\n",
        "        logits_shift.view(-1, logits_shift.size(-1)),\n",
        "        targets_shift.view(-1),\n",
        "        ignore_index=-100,\n",
        "        reduction='none'\n",
        "    )\n",
        "    recon_loss_all = recon_loss_all.view_as(targets_shift)\n",
        "\n",
        "    # Apply mask and sum over valid tokens\n",
        "    recon_loss = (recon_loss_all * mask_shift).sum() / mask_shift.sum().clamp_min(1.0)\n",
        "\n",
        "    # KL(q||p)\n",
        "    # Closed form for diagonal Gaussians\n",
        "    kl_loss = 0.5 * torch.sum(\n",
        "        logvar_prior - logvar_post +\n",
        "        (torch.exp(logvar_post) + (mu_post - mu_prior).pow(2)) / torch.exp(logvar_prior) - 1\n",
        "    )\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = recon_loss + kl_weight * kl_loss\n",
        "\n",
        "    return total_loss, recon_loss, kl_loss"
      ],
      "metadata": {
        "id": "oULl7MaXzi_v"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "\n",
        "## Training Function: `train_conditional_improved`\n",
        "\n",
        "**Goal:**  \n",
        "Train the Conditional VAE with better stability and control over the KL term.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Main Features\n",
        "  \n",
        " **β-annealing** — slowly increases KL weight  \n",
        " **Cosine LR decay** — smooth learning rate drop  \n",
        " **Gradient accumulation** — handles large batches  \n",
        " **Early stopping** — stops when no improvement  \n",
        "\n",
        "---\n",
        "\n",
        "###  Training Steps\n",
        "1. **Forward pass**\n",
        "   - Encode `[source + target]` → `mu_post`, `logvar_post`\n",
        "   - Encode `[source]` → `mu_prior`, `logvar_prior`\n",
        "   - Sample `z = mu_post + exp(0.5 * logvar_post) * eps`\n",
        "   - Decode `[source + target]` and `z` → `logits`\n",
        "\n",
        "2. **Loss computation**\n",
        "   - Mask out source tokens\n",
        "   - Compute:\n",
        "     ```\n",
        "     total = recon_loss + β * kl_loss\n",
        "     ```\n",
        "   - Use `conditional_vae_loss()` for both terms\n",
        "\n",
        "3. **Backward & optimization**\n",
        "   - Accumulate gradients (optional)\n",
        "   - Clip to 1.0 for stability\n",
        "   - Update weights and learning rate\n",
        "\n",
        "4. **Validation & saving**\n",
        "   - Evaluate after each epoch\n",
        "   - Save best model when validation loss improves\n",
        "   - Stop early if no improvement for `patience` epochs\n",
        "\n",
        "---\n",
        "\n",
        "##  Understanding Beta Annealing in VAEs\n",
        "\n",
        "Beta annealing is a key training technique for VAEs that helps prevent common failure modes and encourages the model to learn meaningful representations.\n",
        "\n",
        "### **What is Beta Annealing?**\n",
        "\n",
        "It's a training strategy that dynamically adjusts the weight ($β$) of the KL divergence loss term during optimization. Think of it as putting training wheels on the model.\n",
        "\n",
        "> The process involves gradually increasing $β$ from a small initial value to a target value (typically 1.0).\n",
        ">\n",
        "> 1.  **Start Low** (e.g., $β \\approx 0.01$): The model first prioritizes **reconstruction**, learning to build a strong and informative latent space ($z$).\n",
        "> 2.  **Increase Gradually** ($β \\to 1.0$): The regularization penalty is slowly introduced, encouraging the latent space to become smooth and well-structured.\n",
        "\n",
        "This gradual approach strikes a balance between accurate reconstruction and effective regularization.\n",
        "\n",
        "---\n",
        "\n",
        "###  β-Annealing Schedule\n",
        "| Phase | β value | Purpose |\n",
        "|--------|----------|----------|\n",
        "| Early training | `β = 0.01` | Focus on reconstruction |\n",
        "| Later epochs | Gradually increase to `β = 0.8` | Encourage latent learning |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### **Why is it Necessary? The Problem of Posterior Collapse**\n",
        "\n",
        "The primary reason for using beta annealing is to prevent **posterior collapse**.\n",
        "\n",
        "This is a critical failure mode where the model learns to **ignore the latent variable ($z$)**. The decoder essentially becomes a deterministic function, copying the input rather than learning to generate diverse data from a meaningful latent space.\n",
        "\n",
        " **The Goal:** Beta annealing forces the model to encode useful information in $z$, ensuring it learns a rich, generative representation.\n",
        "\n",
        "---\n",
        "\n",
        "### ** Practical Implementation: The `cycle_iters` Hyperparameter**\n",
        "\n",
        "The `cycle_iters` hyperparameter controls the annealing schedule, often allowing for cyclical patterns.\n",
        "\n",
        "* **Cyclical Annealing (`cycle_iters > 0`)**\n",
        "    * The schedule is repeated over a set number of iterations (e.g., 8-12 cycles per epoch).\n",
        "    * Each cycle ramps $β$ from low to high, creating a sawtooth pattern. This stabilizes training by repeatedly allowing the model to focus on reconstruction before re-applying the regularization pressure.\n",
        "\n",
        "* **Constant Beta (`cycle_iters = 0`)**\n",
        "    * This setting usually defaults to a constant $β=1.0$ throughout training.\n",
        "    * While simpler, it's riskier and can lead to posterior collapse if the model isn't able to learn a good latent space quickly.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "###  Logged During Training\n",
        "- Total, reconstruction, and KL losses  \n",
        "\n",
        "- Learning rate and β values  \n",
        "\n",
        "---\n",
        "\n",
        "###  Output\n",
        "Returns:\n",
        "- `train_losses` — total, recon, KL per epoch  \n",
        "- `val_losses` — total, recon, KL per epoch\n"
      ],
      "metadata": {
        "id": "Ix5FRY95o-IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gFFfNBSaCX6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_conditional_improved(model, train_loader, valid_loader, num_epochs=20,\n",
        "                              learning_rate=5e-4, cycle_iters=8,\n",
        "                              tokenizer=None, save_path='checkpoints',\n",
        "                              gradient_accumulation_steps=1, patience=5,\n",
        "                              source_len=20, use_dynamic_source=False):\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_loader))\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    train_losses = {'total': [], 'recon': [], 'kl': []}\n",
        "    val_losses = {'total': [], 'recon': [], 'kl': []}\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss_train = 0\n",
        "        total_recon_train = 0\n",
        "        total_kl_train = 0\n",
        "        num_batches = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            seq_len = input_ids.size(1)\n",
        "\n",
        "            current_source_len = source_len\n",
        "            if use_dynamic_source:\n",
        "                 # Randomly vary source length for each batch\n",
        "                 # Ensure source_len is at least 1 and less than seq_len\n",
        "                 current_source_len = random.randint(1, seq_len - 1)\n",
        "\n",
        "\n",
        "            # Beta annealing\n",
        "            beta = 1.0 # Default beta\n",
        "\n",
        "            if cycle_iters > 0:\n",
        "                # Calculate beta based on position within the cycle\n",
        "                total_iters_per_epoch = len(train_loader)\n",
        "                iter_in_epoch = i + 1\n",
        "                cycle_len = total_iters_per_epoch / cycle_iters\n",
        "                iter_in_cycle = iter_in_epoch % cycle_len if cycle_len > 0 else 0\n",
        "                beta = min(1.0, iter_in_cycle / cycle_len) # Linear annealing within cycle\n",
        "                beta = 0.8 * beta + 0.01 # Anneal from 0.01 to 0.8 (or similar range)\n",
        "\n",
        "\n",
        "            logits, mu_post, logvar_post, _, full_mask, mu_prior, logvar_prior = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                source_len=current_source_len\n",
        "            )\n",
        "\n",
        "            # Compute loss, only on the target part\n",
        "            target_mask = full_mask.clone()\n",
        "            target_mask[:, :current_source_len] = 0 # Mask out source tokens for reconstruction\n",
        "\n",
        "\n",
        "            total_loss, recon_loss, kl_loss = conditional_vae_loss(\n",
        "                logits, input_ids, target_mask, mu_post, logvar_post, mu_prior, logvar_prior, kl_weight=beta\n",
        "            )\n",
        "\n",
        "            # Backpropagate\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (i + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss_train += total_loss.item()\n",
        "            total_recon_train += recon_loss.item()\n",
        "            total_kl_train += kl_loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Clear output periodically to show progress cleanly\n",
        "            if (i + 1) % 100 == 0:\n",
        "                 clear_output(wait=True)\n",
        "\n",
        "        # Step optimizer and scheduler for remaining gradients\n",
        "        if (num_batches % gradient_accumulation_steps != 0):\n",
        "             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "             optimizer.step()\n",
        "             scheduler.step()\n",
        "             optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = total_loss_train / num_batches\n",
        "        avg_recon_train = total_recon_train / num_batches\n",
        "        avg_kl_train = total_kl_train / num_batches\n",
        "\n",
        "        train_losses['total'].append(avg_train_loss)\n",
        "        train_losses['recon'].append(avg_recon_train)\n",
        "        train_losses['kl'].append(avg_kl_train)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        total_loss_val, total_recon_val, total_kl_val = evaluate_conditional_improved(\n",
        "             model, valid_loader, kl_weight=1.0, source_len=source_len # Use constant beta for validation\n",
        "        )\n",
        "\n",
        "        val_losses['total'].append(total_loss_val)\n",
        "        val_losses['recon'].append(total_recon_val)\n",
        "        val_losses['kl'].append(total_kl_val)\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Train Recon: {avg_recon_train:.4f}, Train KL: {avg_kl_train:.4f}\")\n",
        "        print(f\"                Valid Loss: {total_loss_val:.4f}, Valid Recon: {total_recon_val:.4f}, Valid KL: {total_kl_val:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if total_loss_val < best_val_loss:\n",
        "            best_val_loss = total_loss_val\n",
        "            epochs_without_improvement = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "            }, f'{save_path}/best_model.pth')\n",
        "            print(f\"Saved best model at epoch {epoch+1} with validation loss: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            print(f\"Validation loss did not improve for {epochs_without_improvement} epochs.\")\n",
        "\n",
        "\n",
        "        # Early stopping\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping after {patience} epochs without improvement.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "_Iqx3ksjV66M"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "## Function: `evaluate_conditional_improved`\n",
        "\n",
        "**Goal:**  \n",
        "Check how well the model performs on the validation set"
      ],
      "metadata": {
        "id": "kN8DwP7GpMHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_conditional_improved(model, dataloader, kl_weight, source_len):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_recon = 0\n",
        "    total_kl = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            logits, mu_post, logvar_post, _, full_mask, mu_prior, logvar_prior = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                source_len=source_len # Use fixed source length for evaluation\n",
        "            )\n",
        "\n",
        "            # Compute loss, only on the target part\n",
        "            target_mask = full_mask.clone()\n",
        "            target_mask[:, :source_len] = 0 # Mask out source tokens for reconstruction\n",
        "\n",
        "            loss, recon, kl = conditional_vae_loss(\n",
        "                logits, input_ids, target_mask, mu_post, logvar_post, mu_prior, logvar_prior, kl_weight=kl_weight\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_recon += recon.item()\n",
        "            total_kl += kl.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches, total_recon / num_batches, total_kl / num_batches"
      ],
      "metadata": {
        "id": "vJZIOkROSXYW"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Utilities\n",
        "\n",
        "### `generate_conditional_fixed(...)`\n",
        "- **Purpose:** Generate one continuation from a source prompt using a **fixed** latent `z` from the prior `p(z|c)` (often `z = μ_prior`).\n",
        "- **How it works:**\n",
        "  1) Tokenize source → get `μ_prior, logσ²_prior` from the prior network.  \n",
        "  2) Set `z` (no resampling).  \n",
        "  3) Autoregressively sample next tokens from the decoder with `z` added at every step.  \n",
        "  4) Apply sampling controls: temperature, top-k, top-p, repetition penalty.  \n",
        "  5) Stop at EOS or max length and detokenize.\n",
        "- **Use when:** You want a **stable** single output tied closely to the source.\n",
        "\n",
        "### `generate_diverse_samples(...)`\n",
        "- **Purpose:** Generate **multiple** continuations by sampling **different** `z` values from `p(z|c)` to induce diversity.\n",
        "- **How it works:**\n",
        "  1) Tokenize source → compute `μ_prior, logσ²_prior` once.  \n",
        "  2) For each sample, draw a new `z ~ N(μ_prior, σ_prior² I)`.  \n",
        "  3) Autoregressively decode with the same sampling controls.  \n",
        "  4) Collect `num_samples` detokenized outputs.\n",
        "- **Use when:** You want a **set** of varied candidates reflecting different plausible latents."
      ],
      "metadata": {
        "id": "_3Js4r9TqMUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conditional_fixed(model, tokenizer, source_text, max_length=100,\n",
        "                               temperature=1.0, top_k=50, top_p=0.95,\n",
        "                               repetition_penalty=1.2, device='cuda'):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    Args:\n",
        "        model: Trained conditional VAE\n",
        "        tokenizer: Tokenizer\n",
        "        source_text: Source/prompt text\n",
        "        max_length: Max tokens to generate\n",
        "        temperature: Sampling temperature (higher = more random)\n",
        "        top_k: Top-k sampling (0 = disabled)\n",
        "        top_p: Nucleus sampling threshold\n",
        "        repetition_penalty: Penalty for repeated tokens (>1.0 discourages repetition)\n",
        "        device: Device\n",
        "\n",
        "    Returns:\n",
        "        Generated text\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode source\n",
        "        encoded = tokenizer(source_text, return_tensors='pt',\n",
        "                          padding=True, truncation=True, max_length=512)\n",
        "        source_ids = encoded['input_ids'].to(device)\n",
        "        source_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "        # Sample z from prior p(z|c)\n",
        "        mu_prior, logvar_prior = model.prior_net(source_ids, source_mask)\n",
        "        z =  mu_prior#model.reparameterize(mu_prior, logvar_prior)\n",
        "\n",
        "        # Start with source tokens\n",
        "        generated = source_ids.clone()\n",
        "        source_len = source_ids.size(1)\n",
        "\n",
        "        for step in range(max_length):\n",
        "            # Get logits for full sequence\n",
        "            logits = model.decoder(generated, z)\n",
        "\n",
        "            # Get next token logits\n",
        "            next_logits = logits[:, -1, :].clone()\n",
        "\n",
        "            # Apply repetition penalty\n",
        "            if repetition_penalty != 1.0:\n",
        "                for token_id in set(generated[0].tolist()):\n",
        "                    # If score < 0, multiply by penalty (make more negative)\n",
        "                    # If score > 0, divide by penalty (make less positive)\n",
        "                    if next_logits[0, token_id] < 0:\n",
        "                        next_logits[0, token_id] *= repetition_penalty\n",
        "                    else:\n",
        "                        next_logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "            # Apply temperature\n",
        "            next_logits = next_logits / temperature\n",
        "\n",
        "            # Top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = next_logits < torch.topk(next_logits, min(top_k, next_logits.size(-1)))[0][..., -1, None]\n",
        "                next_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Top-p (nucleus) filtering\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True, dim=-1)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                # Remove tokens with cumulative probability above threshold\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                # Keep at least one token\n",
        "                sorted_indices_to_remove[..., 0] = False\n",
        "\n",
        "                # Scatter back to original indexing\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                next_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample\n",
        "            probs = F.softmax(next_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if hasattr(tokenizer, 'eos_token_id') and next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Append token\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "            # Stop if max length reached\n",
        "            if generated.size(1) >= source_len + max_length:\n",
        "                break\n",
        "\n",
        "        # Decode only the generated part (skip source)\n",
        "        generated_tokens = generated[0, source_len:].cpu().tolist()\n",
        "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "\n",
        "def generate_diverse_samples(model, tokenizer, source_text, num_samples=5,\n",
        "                            max_length=100, temperature=1.0, top_k=50,\n",
        "                            top_p=0.95, repetition_penalty=1.2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generate multiple diverse samples by sampling different z values.\n",
        "\n",
        "    Args:\n",
        "        model: Trained conditional VAE\n",
        "        tokenizer: Tokenizer\n",
        "        source_text: Source text\n",
        "        num_samples: Number of samples to generate\n",
        "        max_length: Max generation length\n",
        "        temperature: Sampling temperature\n",
        "        top_k: Top-k sampling\n",
        "        top_p: Nucleus sampling\n",
        "        repetition_penalty: Repetition penalty\n",
        "        device: Device\n",
        "\n",
        "    Returns:\n",
        "        List of generated texts\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode source once\n",
        "        encoded = tokenizer(source_text, return_tensors='pt',\n",
        "                          padding=True, truncation=True, max_length=512)\n",
        "        source_ids = encoded['input_ids'].to(device)\n",
        "        source_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "        # Get prior distribution\n",
        "        mu_prior, logvar_prior = model.prior_net(source_ids, source_mask)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Sample different z each time\n",
        "            z = model.reparameterize(mu_prior, logvar_prior)\n",
        "\n",
        "            # Generate with this z\n",
        "            generated = source_ids.clone()\n",
        "            source_len = source_ids.size(1)\n",
        "\n",
        "            for step in range(max_length):\n",
        "                logits = model.decoder(generated, z)\n",
        "                next_logits = logits[:, -1, :].clone()\n",
        "\n",
        "                # Repetition penalty\n",
        "                if repetition_penalty != 1.0:\n",
        "                    for token_id in set(generated[0].tolist()):\n",
        "                        if next_logits[0, token_id] < 0:\n",
        "                            next_logits[0, token_id] *= repetition_penalty\n",
        "                        else:\n",
        "                            next_logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "                next_logits = next_logits / temperature\n",
        "\n",
        "                # Top-k\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_logits < torch.topk(next_logits, min(top_k, next_logits.size(-1)))[0][..., -1, None]\n",
        "                    next_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "                # Top-p\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_logits, descending=True, dim=-1)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    sorted_indices_to_remove[..., 0] = False\n",
        "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                    next_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "                # Sample\n",
        "                probs = F.softmax(next_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                if hasattr(tokenizer, 'eos_token_id') and next_token.item() == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "                generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "                if generated.size(1) >= source_len + max_length:\n",
        "                    break\n",
        "\n",
        "            # Decode\n",
        "            generated_tokens = generated[0, source_len:].cpu().tolist()\n",
        "            text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "            samples.append(text)\n",
        "\n",
        "    return samples"
      ],
      "metadata": {
        "id": "CZATuSgiRpCc"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and generation"
      ],
      "metadata": {
        "id": "c-KonnRzqmBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'vocab_size': len(tokenizer), 'embed_dim': 256, 'latent_dim': 64}\n",
        "model = RegaVAE(\n",
        "    vocab_size=len(tokenizer),\n",
        "    embed_dim=256,\n",
        "    latent_dim=128,  # Larger latent space\n",
        "    use_retrieval=False\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "train_losses, val_losses = train_conditional_improved(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    valid_loader=valid_loader,\n",
        "    num_epochs=22,\n",
        "    learning_rate=5e-4,\n",
        "    cycle_iters=11,\n",
        "    tokenizer=tokenizer,\n",
        "    save_path='improved_checkpoints',\n",
        "    gradient_accumulation_steps=1,\n",
        "    patience=3,\n",
        "    source_len=20\n",
        ")\n",
        "\n",
        "checkpoint = torch.load('improved_checkpoints/best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "source = \"The movie was\"\n",
        "generated = generate_conditional_fixed(\n",
        "    model, tokenizer, source,\n",
        "    max_length=20,\n",
        "    temperature=1.0,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2,\n",
        "    device=device\n",
        ")\n",
        "print(f\"Source: {source}\")\n",
        "print(f\"Generated: {generated}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DIVERSE SAMPLES (Different z)\")\n",
        "print(\"=\" * 60)\n",
        "samples = generate_diverse_samples(\n",
        "    model, tokenizer, source,\n",
        "    num_samples=5,\n",
        "    max_length=50,\n",
        "    temperature=1.0,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2,\n",
        "    device=device\n",
        ")\n",
        "for i, sample in enumerate(samples, 1):\n",
        "    print(f\"\\n{i}. {sample}\")\n"
      ],
      "metadata": {
        "id": "0vIp0RVnTI3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b23cc7b-2e4b-4eec-c64d-2d57d2c822a3"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/22 (Train): 100%|██████████| 196/196 [00:17<00:00, 11.39it/s]\n",
            "Evaluating: 100%|██████████| 20/20 [00:01<00:00, 14.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/22: Train Loss: 4.9306, Train Recon: 4.5598, Train KL: 0.9072\n",
            "                Valid Loss: 4.9279, Valid Recon: 4.7751, Valid KL: 0.1528\n",
            "Validation loss did not improve for 1 epochs.\n",
            "Source: The movie was\n",
            "Generated:  n't . i can i say that it . ? what to eat is a _UNK !\n",
            "\n",
            "============================================================\n",
            "DIVERSE SAMPLES (Different z)\n",
            "============================================================\n",
            "\n",
            "1.  not sure about it can go a _UNK or i need the real time on any year but now that it is called _UNK to give me when a _UNK , i 'm having a day of a child . is one person\n",
            "\n",
            "2.  going to go on ? i 'm not really ! i do what happens in the _UNK . the first _UNK of its only that we are doing up there is a _UNK ( and is not be good to see your muscles\n",
            "\n",
            "3.  still to it ? when you would ! no way up ... ... what it for a child . _UNK and it has it out the first time and i have any two weeks my mom , and my head _UNK and im not my question\n",
            "\n",
            "4.  the _UNK and _UNK and he is there ! i think about it ? it could be in one to give us for an interesting weight _UNK _UNK , _UNK with god . just , then `` a great _\n",
            "\n",
            "5.  an electric . i am not been just like to go for it is so good for some ? ? i 've heard and _UNK _UNK to hear what a _UNK ? does anyone know if you look at her _UNK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##  END OF PART 1\n",
        "---\n",
        "\n",
        "### Next: Part 2 will cover\n",
        "- Building the retrieval database  \n",
        "- Implementing retrieval-augmented generation  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AyR0crdSq2R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAISS: What It Is and Why We Use It\n",
        "\n",
        "- **FAISS** is a fast library for **similarity search** between vectors.\n",
        "- You give it many vectors (like hidden or latent features), and it builds an **index**.\n",
        "- Later, you can query this index with a new vector to find the **most similar** ones.\n",
        "- Common similarity measures:\n",
        "  - **Cosine similarity** → compares direction (angle) between vectors.\n",
        "  - **L2 distance** → compares Euclidean distance (straight-line distance).\n",
        "\n",
        "---\n",
        "\n",
        "### What the Following Code Does\n",
        "\n",
        "####  `RetrievalDatabase` Class\n",
        "\n",
        "This class wraps FAISS to store and search for similar latent vectors.\n",
        "\n",
        "- **Initialization (`__init__`)**\n",
        "  - Sets the latent vector size (`latent_dim`) and similarity type (`metric`).\n",
        "  - Builds a FAISS index:\n",
        "    - `IndexFlatIP` → for cosine similarity (after normalization)\n",
        "    - `IndexFlatL2` → for Euclidean distance\n",
        "\n",
        "- **`add_items(latent_vectors, texts)`**\n",
        "  - Normalizes the vectors if using cosine similarity.\n",
        "  - Adds them to the FAISS index.\n",
        "  - Stores the matching texts in a list.\n",
        "\n",
        "- **`search(query_vectors, k=5)`**\n",
        "  - Normalizes queries if using cosine metric.\n",
        "  - Searches for top `k` most similar vectors.\n",
        "  - Returns:\n",
        "    - `retrieved_texts`: texts of the most similar items\n",
        "    - `distances`: similarity or distance scores\n",
        "    - `weights`: softmax-normalized similarities (used as attention weights)\n",
        "\n",
        "- **`save(path)` / `load(path)`**\n",
        "  - Saves or loads the FAISS index and associated text data.\n",
        "\n",
        "---\n",
        "\n",
        "####  `build_retrieval_database(model, dataloader, tokenizer, latent_dim)`\n",
        "\n",
        "This function builds a retrieval database from your training data.\n",
        "\n",
        "- Steps:\n",
        "  1. Use the model’s **encoder** to get a latent vector (`mu`) for each training example.\n",
        "  2. Decode the input IDs back to text.\n",
        "  3. Add all latent vectors and texts to the FAISS index.\n",
        "\n",
        "- Returns: a ready-to-use `RetrievalDatabase` that can be queried later.\n",
        "\n",
        "---\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Acts as a **memory bank** of training examples in latent space.\n",
        "- During generation, the model can **retrieve similar examples** based on latent similarity.\n",
        "- The retrieved examples and weights are used to **improve context and generation quality** .\n"
      ],
      "metadata": {
        "id": "qJODTDVVrcGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##  How FAISS Enables Fast Similarity Search\n",
        "\n",
        "This snippet demonstrates **FAISS** (Facebook AI Similarity Search), a library designed for efficient similarity search and clustering of dense vectors. It highlights the fundamental trade-off between search speed and accuracy by comparing two common index types.\n",
        "\n",
        "### **1. Exact Search (`IndexFlatL2`)**\n",
        "\n",
        "This is the most straightforward approach. It acts as our \"ground truth\" by performing an **exhaustive search**.\n",
        "\n",
        "* **How it works:** It computes the distance between a query vector and *every other vector* in the index.\n",
        "* **Result:** It guarantees finding the exact nearest neighbors.\n",
        "* **Downside:** This becomes incredibly slow as the number of vectors grows into the millions or billions.\n",
        "\n",
        "### **2. Approximate Search (`IndexIVFFlat`)**\n",
        "\n",
        "This method trades a small amount of accuracy for a massive gain in speed. It's an **approximate nearest neighbor** (ANN) search technique.\n",
        "\n",
        "* **How it works:**\n",
        "    1.  **Partitioning:** The vector space is first divided into partitions, or \"cells.\" A representative vector (a centroid) is chosen for each cell.\n",
        "    2.  **Searching:** When a query vector comes in, FAISS identifies the few cells closest to it and only searches within that small subset of vectors.\n",
        "* **Result:** It finds neighbors that are *very likely* to be the true nearest ones, but without a perfect guarantee.\n",
        "* **Upside:** The search time is dramatically reduced because it avoids comparing against the entire dataset.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7eALq9J_QbhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np, faiss, time\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "d = 64\n",
        "n = 20_000\n",
        "k = 5\n",
        "\n",
        "xb = rng.standard_normal((n, d)).astype('float32')\n",
        "xb /= np.linalg.norm(xb, axis=1, keepdims=True)\n",
        "xq = rng.standard_normal((10, d)).astype('float32')\n",
        "xq /= np.linalg.norm(xq, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "index_exact = faiss.IndexFlatIP(d)\n",
        "index_exact.add(xb)\n",
        "\n",
        "t0 = time.time()\n",
        "D_exact, I_exact = index_exact.search(xq, k)\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"EXACT: {n} vecs, {k}-NN for {len(xq)} queries in {(t1 - t0)*1e3:.1f} ms\")\n",
        "print(\"Top-5 neighbors for q0 (exact):\", I_exact[0].tolist())\n",
        "\n",
        "\n",
        "nlist = 100\n",
        "quantizer = faiss.IndexFlatIP(d)\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "\n",
        "assert not index_ivf.is_trained\n",
        "index_ivf.train(xb)\n",
        "index_ivf.add(xb)\n",
        "index_ivf.nprobe = 8\n",
        "\n",
        "t0 = time.time()\n",
        "D_ivf, I_ivf = index_ivf.search(xq, k)\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"IVF:   {n} vecs, {k}-NN for {len(xq)} queries in {(t1 - t0)*1e3:.1f} ms\")\n",
        "print(\"Top-5 neighbors for q0 (ivf):  \", I_ivf[0].tolist())\n",
        "\n",
        "overlap = (I_ivf[:, :k][:, None, :] == I_exact[:, :k][:, :, None]).any(-1).mean()\n",
        "print(f\"Recall@{k} vs exact: {overlap*100:.1f}%  (increase nprobe for higher recall)\")\n"
      ],
      "metadata": {
        "id": "aTgHXKOxQilz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424b482e-639f-40f6-e977-2125bf03a7a4"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXACT: 20000 vecs, 5-NN for 10 queries in 3.6 ms\n",
            "Top-5 neighbors for q0 (exact): [7596, 1704, 11143, 6527, 3196]\n",
            "IVF:   20000 vecs, 5-NN for 10 queries in 0.5 ms\n",
            "Top-5 neighbors for q0 (ivf):   [1704, 11651, 6735, 14328, 12159]\n",
            "Recall@5 vs exact: 34.0%  (increase nprobe for higher recall)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Key Takeaway**\n",
        "\n",
        "> This comparison is essential for understanding why FAISS is so powerful. For applications like **text or image retrieval**, where you need to search through millions of embeddings in real-time, the speed of an approximate index like IVF is crucial, and the tiny loss in accuracy is an acceptable trade-off."
      ],
      "metadata": {
        "id": "SRX05kfkQ-Hk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "FRtYD1OXJCYx"
      },
      "outputs": [],
      "source": [
        "class RetrievalDatabase:\n",
        "    def __init__(self, latent_dim=32, metric='cosine'):\n",
        "        \"\"\"\n",
        "        FAISS index for latent vectors.\n",
        "        CONCEPT: Index latents from train corpus; query with z_x for similar 'examples'.\n",
        "                 Cosine: Semantic similarity (angle, not magnitude); weights guide fusion.\n",
        "\n",
        "        \"\"\"\n",
        "        self.latent_dim = latent_dim\n",
        "        self.metric = metric\n",
        "        self.texts = []\n",
        "        self.index = None\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self):\n",
        "        if self.metric == 'cosine':\n",
        "            self.index = faiss.IndexFlatIP(self.latent_dim)  # Inner product for cosine (after norm)\n",
        "        else:  # l2 default\n",
        "            self.index = faiss.IndexFlatL2(self.latent_dim)\n",
        "\n",
        "    def add_items(self, latent_vectors, texts):\n",
        "        # Normalize for cosine\n",
        "        if self.metric == 'cosine':\n",
        "            norms = np.linalg.norm(latent_vectors, axis=1, keepdims=True)\n",
        "            latent_vectors = latent_vectors / (norms + 1e-8)\n",
        "\n",
        "        self.index.add(latent_vectors.astype('float32'))\n",
        "        self.texts.extend(texts)\n",
        "        print(f\"Added {len(texts)} items; total in DB: {len(self.texts)}\")\n",
        "\n",
        "    def search(self, query_vectors, k=5):\n",
        "        \"\"\"\n",
        "        Query top-k; return texts, distances, weights (softmax sims).\n",
        "        CONCEPT: Low dist/high sim → relevant 'future contexts'; weights for soft blend.\n",
        "        \"\"\"\n",
        "        if self.metric == 'cosine':\n",
        "            query_norms = np.linalg.norm(query_vectors, axis=1, keepdims=True)\n",
        "            query_vectors = query_vectors / (query_norms + 1e-8)\n",
        "\n",
        "        distances, indices = self.index.search(query_vectors.astype('float32'), k)\n",
        "\n",
        "        retrieved_texts = []\n",
        "        retrieved_indices = []\n",
        "        for idx_list in indices:\n",
        "            texts = [self.texts[i] if 0 <= i < len(self.texts) else \"\" for i in idx_list]\n",
        "            retrieved_texts.append(texts)\n",
        "            retrieved_indices.append(idx_list)\n",
        "\n",
        "        # Weights: Softmax of similarities (1 / (1 + dist) for L2; distances for IP)\n",
        "        if self.metric == 'cosine':\n",
        "            sims = distances  # IP = cosine after norm\n",
        "        else:\n",
        "            sims = 1 / (1 + distances)  # Transform L2 to sim\n",
        "        weights = np.exp(sims) / np.sum(np.exp(sims), axis=1, keepdims=True)  # Softmax per query\n",
        "\n",
        "        return retrieved_texts, distances, weights\n",
        "\n",
        "    def save(self, path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        faiss.write_index(self.index, f\"{path}/faiss.index\")\n",
        "        with open(f\"{path}/texts.json\", 'w') as f:\n",
        "            json.dump(self.texts, f)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.index = faiss.read_index(f\"{path}/faiss.index\")\n",
        "        with open(f\"{path}/texts.json\", 'r') as f:\n",
        "            self.texts = json.load(f)\n",
        "\n",
        "def build_retrieval_database(model, dataloader, tokenizer, latent_dim=32):\n",
        "    \"\"\"\n",
        "    Build DB: Encode train latents → index.\n",
        "    CONCEPT: Static 'corpus memory' from train; query during RegaVAE phase.\n",
        "             Run post-baseline to leverage learned latents.\n",
        "    TODO: Update DB every N epochs (dynamic)—add to train loop; does it improve PPL?\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    retrieval_db = RetrievalDatabase(latent_dim=latent_dim, metric='cosine')\n",
        "\n",
        "    all_latents = []\n",
        "    all_texts = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Building DB\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            mu, _ = model.encoder(input_ids, attention_mask)\n",
        "            all_latents.append(mu.cpu().numpy())\n",
        "\n",
        "            # Decode texts for storage\n",
        "            for ids in input_ids:\n",
        "                text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "                all_texts.append(text)\n",
        "\n",
        "    all_latents = np.vstack(all_latents)\n",
        "    retrieval_db.add_items(all_latents, all_texts)\n",
        "\n",
        "    return retrieval_db\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEED TO FILL\n",
        "### Overview: Training Function `train_regavae`\n",
        "\n",
        "**Purpose:**  \n",
        "This function trains the **Retrieval-Augmented Variational Autoencoder (RegaVAE)**.  \n",
        "It builds upon a normal VAE training loop but adds **retrieval-based memory** and **fine-tuning options** for better diversity and control.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "1. **Fine-tuning the Decoder**\n",
        "   -  **freeze the encoder** so only the decoder learns.\n",
        "   .\n",
        "\n",
        "2. **Retrieval-Augmented Training**\n",
        "   - Each batch uses the encoder output (`mu_post`) to **query the retrieval database**.\n",
        "   - The retrieved examples (and their weights) are then **fused** into the decoder input.\n",
        "   - This adds \"memory\" to training  letting the model learn from related examples.\n",
        "\n",
        "3. **KL Annealing**\n",
        "   - Gradually increases the KL weight `β` during training.\n",
        "   - Starts small and grows within a defined cycle to balance reconstruction and regularization.\n",
        "\n",
        "4. **Dynamic Source Length**\n",
        "   - Optionally vary the split between `source` (input/prompt) and `target` (output/response).\n",
        "   - Helps the model generalize better across different input sizes.\n",
        "\n",
        "5. **Loss Components**\n",
        "   - **Reconstruction loss:** how well the model predicts the target tokens.  \n",
        "   - **KL loss:** how close the latent distribution is to the prior.  \n",
        "   - **Total loss:** combined using the `β` weight.\n",
        "\n",
        "\n",
        "\n",
        "7. **Validation and Checkpointing**\n",
        "   - Runs validation after each epoch using the same retrieval setup.\n",
        "   - Saves model when validation loss improves.\n",
        "   - Stops early if there’s no improvement for several epochs (`patience`).\n",
        "\n",
        "8. **Gradient Handling**\n",
        "   - Supports gradient accumulation for large batches.\n",
        "   - Clips gradients to stabilize training.\n",
        "   - Uses cosine annealing to smoothly adjust the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**MAKE SURE**\n",
        "\n",
        "When filling this code:\n",
        "- **Understand the flow:** Encode → Retrieve → Decode → Compute Loss → Backprop.\n",
        "- **Focus on retrieval logic:** how latent vectors query FAISS, how texts are tokenized and weighted.\n",
        "- **See how training differs from a normal VAE:**\n",
        "  - Retrieval-based context added to decoder.\n",
        "  - Optional encoder freezing.\n",
        "  - Dynamic source and KL annealing.\n",
        "- **Be careful with masking:** only compute reconstruction loss on the target part of the sequence.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### In Short\n",
        "\n",
        "This training loop:\n",
        "1. Encodes  to get latent variables.  \n",
        "2. Retrieves similar examples using FAISS (from latent space).  \n",
        "3. Decodes with retrieved examples to improve generation.  \n",
        "4. Computes losses and updates decoder parameters.  \n",
        "\n"
      ],
      "metadata": {
        "id": "YIN5Tgq5sXvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_regavae(model, train_loader, valid_loader, num_epochs=8,\n",
        "                  learning_rate=1e-4, cycle_iters=12,\n",
        "                  tokenizer=None, save_path='regavae_checkpoints',\n",
        "                  gradient_accumulation_steps=1, patience=5,\n",
        "                  source_len=20, use_dynamic_source=False,\n",
        "                  freeze_encoder=True):  # New param: Freeze baseline encoder\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_loader))\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    train_losses = {'total': [], 'recon': [], 'kl': []}\n",
        "    val_losses = {'total': [], 'recon': [], 'kl': []}\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Freeze encoder and prior if specified\n",
        "    if freeze_encoder:\n",
        "        print(\"Freezing encoder and prior networks...\")\n",
        "        for param in model.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in model.prior_net.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Only decoder parameters will be updated\n",
        "        optimizer = optim.AdamW(model.decoder.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss_train = 0\n",
        "        total_recon_train = 0\n",
        "        total_kl_train = 0\n",
        "        num_batches = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            seq_len = input_ids.size(1)\n",
        "\n",
        "            current_source_len = source_len\n",
        "            if use_dynamic_source:\n",
        "                 # Randomly vary source length for each batch\n",
        "                 # Ensure source_len is at least 1 and less than seq_len\n",
        "                 # Use min(seq_len - 1, max_allowed_source_len) to prevent issues with short sequences\n",
        "                 max_allowed_source_len = min(seq_len - 1, 30) # Example cap\n",
        "                 current_source_len = random.randint(1, max_allowed_source_len)\n",
        "\n",
        "\n",
        "            # Beta annealing\n",
        "            beta = 1.0 # Default beta\n",
        "\n",
        "            if cycle_iters > 0:\n",
        "                # Calculate beta based on position within the cycle\n",
        "                total_iters_per_epoch = len(train_loader)\n",
        "                iter_in_epoch = i + 1\n",
        "                cycle_len = total_iters_per_epoch / cycle_iters\n",
        "                iter_in_cycle = iter_in_epoch % cycle_len if cycle_len > 0 else 0\n",
        "                beta = min(1.0, iter_in_cycle / cycle_len) # Linear annealing within cycle\n",
        "                beta = 0.8 * beta + 0.01 # Anneal from 0.01 to 0.8 (or similar range)\n",
        "\n",
        "            # --- Retrieval Augmentation ---\n",
        "            retrieved_ids = None\n",
        "            retrieval_weights = None\n",
        "            if model.use_retrieval and model.retrieval_db is not None:\n",
        "                # Get mu from the current batch (using the potentially frozen encoder)\n",
        "                with torch.no_grad(): # Ensure encoder is not updated even if not frozen\n",
        "                     mu_post, _ = model.encoder(input_ids, attention_mask)\n",
        "\n",
        "                # Query the retrieval database\n",
        "                query_vectors = mu_post.cpu().numpy()\n",
        "                retrieved_texts, _, weights_np = model.retrieval_db.search(query_vectors, k=5) # k=5 as in RetrievalDatabase\n",
        "\n",
        "                # Tokenize retrieved texts\n",
        "                encoded_list = []\n",
        "                for texts_per_query in retrieved_texts: # Iterate through batch\n",
        "                    encoded_texts = [\n",
        "                        tokenizer(t, max_length=64, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].squeeze(0)\n",
        "                        for t in texts_per_query\n",
        "                    ]\n",
        "                    encoded_list.append(torch.stack(encoded_texts))\n",
        "                retrieved_ids = torch.stack(encoded_list).to(device) # Shape [B, k, max_length]\n",
        "\n",
        "                retrieval_weights = torch.tensor(weights_np, dtype=torch.float).to(device) # Shape [B, k]\n",
        "\n",
        "            # --- Forward pass ---\n",
        "            # Pass retrieved_ids and retrieval_weights to the model's forward pass\n",
        "            logits, mu_post, logvar_post, _, full_mask, mu_prior, logvar_prior = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                source_len=current_source_len,\n",
        "                retrieved_ids=retrieved_ids,       # Pass retrieved IDs\n",
        "                retrieval_weights=retrieval_weights # Pass retrieval weights\n",
        "            )\n",
        "\n",
        "            # Compute loss, only on the target part\n",
        "            target_mask = full_mask.clone()\n",
        "            target_mask[:, :current_source_len] = 0 # Mask out source tokens for reconstruction\n",
        "\n",
        "\n",
        "            total_loss, recon_loss, kl_loss = conditional_vae_loss(\n",
        "                logits, input_ids, target_mask, mu_post, logvar_post, mu_prior, logvar_prior, kl_weight=beta\n",
        "            )\n",
        "\n",
        "            # Backpropagate\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (i + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss_train += total_loss.item()\n",
        "            total_recon_train += recon_loss.item()\n",
        "            total_kl_train += kl_loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Clear output periodically to show progress cleanly\n",
        "            if (i + 1) % 100 == 0:\n",
        "                 clear_output(wait=True)\n",
        "\n",
        "        # Step optimizer and scheduler for remaining gradients\n",
        "        if (num_batches % gradient_accumulation_steps != 0):\n",
        "             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "             optimizer.step()\n",
        "             scheduler.step()\n",
        "             optimizer.zero_grad()\n",
        "\n",
        "        avg_train_loss = total_loss_train / num_batches\n",
        "        avg_recon_train = total_recon_train / num_batches\n",
        "        avg_kl_train = total_kl_train / num_batches\n",
        "\n",
        "        train_losses['total'].append(avg_train_loss)\n",
        "        train_losses['recon'].append(avg_recon_train)\n",
        "        train_losses['kl'].append(avg_kl_train)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        # Use constant beta=1.0 and fixed source_len for consistent evaluation\n",
        "        model.eval()\n",
        "        total_loss_val, total_recon_val, total_kl_val = evaluate_regavae(\n",
        "             model, valid_loader, kl_weight=1.0, source_len=source_len, tokenizer=tokenizer # Need tokenizer for retrieval in eval\n",
        "        )\n",
        "\n",
        "        val_losses['total'].append(total_loss_val)\n",
        "        val_losses['recon'].append(total_recon_val)\n",
        "        val_losses['kl'].append(total_kl_val)\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Train Recon: {avg_recon_train:.4f}, Train KL: {avg_kl_train:.4f}\")\n",
        "        print(f\"                Valid Loss: {total_loss_val:.4f}, Valid Recon: {total_recon_val:.4f}, Valid KL: {total_kl_val:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if total_loss_val < best_val_loss:\n",
        "            best_val_loss = total_loss_val\n",
        "            epochs_without_improvement = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "            }, f'{save_path}/best_model.pth')\n",
        "            print(f\"Saved best model at epoch {epoch+1} with validation loss: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            print(f\"Validation loss did not improve for {epochs_without_improvement} epochs.\")\n",
        "\n",
        "\n",
        "        # Early stopping\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping after {patience} epochs without improvement.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def evaluate_regavae(model, dataloader, kl_weight, source_len, tokenizer):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_recon = 0\n",
        "    total_kl = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating RegaVAE\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # --- Retrieval Augmentation (for evaluation) ---\n",
        "            retrieved_ids = None\n",
        "            retrieval_weights = None\n",
        "            if model.use_retrieval and model.retrieval_db is not None:\n",
        "                 mu_post, _ = model.encoder(input_ids, attention_mask) # Use encoder to get query\n",
        "\n",
        "                 query_vectors = mu_post.cpu().numpy()\n",
        "                 retrieved_texts, _, weights_np = model.retrieval_db.search(query_vectors, k=5) # k=5\n",
        "\n",
        "                 encoded_list = []\n",
        "                 for texts_per_query in retrieved_texts:\n",
        "                     encoded_texts = [\n",
        "                         tokenizer(t, max_length=64, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].squeeze(0)\n",
        "                         for t in texts_per_query\n",
        "                     ]\n",
        "                     encoded_list.append(torch.stack(encoded_texts))\n",
        "                 retrieved_ids = torch.stack(encoded_list).to(device)\n",
        "\n",
        "                 retrieval_weights = torch.tensor(weights_np, dtype=torch.float).to(device)\n",
        "            # --- End Retrieval Augmentation ---\n",
        "\n",
        "            logits, mu_post, logvar_post, _, full_mask, mu_prior, logvar_prior = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                source_len=source_len,\n",
        "                retrieved_ids=retrieved_ids,       # Pass retrieved IDs\n",
        "                retrieval_weights=retrieval_weights # Pass retrieval weights\n",
        "            )\n",
        "\n",
        "            # Compute loss, only on the target part\n",
        "            target_mask = full_mask.clone()\n",
        "            target_mask[:, :source_len] = 0 # Mask out source tokens for reconstruction\n",
        "\n",
        "            loss, recon, kl = conditional_vae_loss(\n",
        "                logits, input_ids, target_mask, mu_post, logvar_post, mu_prior, logvar_prior, kl_weight=kl_weight\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_recon += recon.item()\n",
        "            total_kl += kl.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches, total_recon / num_batches, total_kl / num_batches"
      ],
      "metadata": {
        "id": "IcmqFANcpRp4"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_db = build_retrieval_database(\n",
        "    model=model,  # Baseline VAE\n",
        "    dataloader=train_loader,\n",
        "    tokenizer=tokenizer,\n",
        "    latent_dim=128\n",
        ")\n",
        "retrieval_db.save('retrieval_db')  # Persist\n",
        "print(\"Retrieval DB built and saved!\")"
      ],
      "metadata": {
        "id": "aB7dNnpLjikt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af80fd9-578b-48a0-8a33-ab84b98f83cf"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building DB: 100%|██████████| 196/196 [00:15<00:00, 12.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 25000 items; total in DB: 25000\n",
            "Retrieval DB built and saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Need to fill\n",
        "\n",
        "### RetrievalAugmentedDecoder function\n",
        "\n",
        " The `RetrievalAugmentedDecoder` class extends the baseline `ConditionalDecoder` by **fusing retrieved examples** into the generation process. This \"augmentation\" injects relevant memory from the training corpus, helping the model generate more diverse and contextually rich text without retraining everything from scratch.\n",
        "\n",
        "**High-Level Idea:**\n",
        "- During training/generation, we retrieve k=5 similar texts (via FAISS on latents).\n",
        "- **Encode** each retrieved text into a fixed embedding (pooled representation).\n",
        "- **Aggregate** them with the latent `z` using attention (z \"queries\" the retrieved \"keys\").\n",
        "- **Fuse** the aggregated info as a lightweight bias added to the main decoder's embeddings.\n",
        "- If no retrieval, it falls back to the baseline decoder (no change!).\n",
        "\n",
        "This keeps things efficient: No heavy concatenation or extra layers—just a soft \"nudge\" to the decoder.\n",
        "\n",
        "**Why This Design?**\n",
        "- **Lightweight:** Add bias instead of concat (saves params/compute).\n",
        "- **Flexible:** Works with baseline code; retrieval optional.\n",
        "- **Interpretable:** Attention lets z focus on useful retrieved parts.\n",
        "\n",
        "Now, let's break it down **method by method**.\n",
        "\n",
        "#### 1. `__init__(self, config)`: Setting Up Components\n",
        "This initializes the decoder like the baseline but adds retrieval-specific parts. It's modular: Reuse baseline embeddings/blocks, add mini-modules for retrieval.\n",
        "\n",
        "**What it does:**\n",
        "- Store config dims .\n",
        "- Set `num_retrievals=5` (k from DB search—tweakable!).\n",
        "- **Baseline parts:** Token/pos embeds, dropout (shared).\n",
        "- **Fusion projections:** Linear layers to match dims (z → embed_dim, agg → embed_dim).\n",
        "- **Retrieval encoder:** A \"mini-transformer\" (1 layer) to summarize each retrieved text.\n",
        "- **Aggregation:** Reuse `SimpleAttention` for z ↔ retrieved fusion.\n",
        "- **Main decoder:** Stacked blocks + norm + LM head (identical to baseline).\n",
        "- Call `_init_weights()` for stable training.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. `_init_weights(self)`: Initialization Helper\n",
        "Standard PyTorch init—nothing fancy, but crucial for VAE stability (avoids exploding/vanishing grads).\n",
        "\n",
        "**What it does:**\n",
        "- Loop over all submodules.\n",
        "- Linear: Normal(0, 0.02) weights, zero biases.\n",
        "- Embeddings: Normal(0, 0.02) weights.\n",
        "\n",
        "**Why?** Matches GPT-style init; keeps signals flowing in transformers.\n",
        "\n",
        "**Coding Tips:**\n",
        "- Call from `__init__`—easy!\n",
        "\n",
        "\n",
        "#### 3. `encode_retrieved(self, retrieved_ids, retrieved_mask=None)`: Summarizing Retrieved Texts\n",
        "This \"mini-encodes\" each of the k retrieved texts into a single vector per text (pooled embedding).\n",
        "\n",
        "**What it does (step-by-step):**\n",
        "1. **Flatten batch:** `retrieved_ids` is [B, k, T] (batch_size, num_retrievals=5, seq_len=64). Reshape to [B*k, T] for batch processing.\n",
        "2. **Embed + Pos:** Token embed + positional (like encoder). Apply dropout.\n",
        "3. **Mini-Transformer:** One `SimpleBlock` (attn + MLP) with causal mask (future tokens ignored, but pooling anyway).\n",
        "4. **LayerNorm:** Stabilize post-block.\n",
        "5. **Pool:** Mean over seq dim → [B*k, n_embd].\n",
        "6. **Reshape:** Back to [B, k, n_embd] (one vec per retrieved text).\n",
        "\n",
        "\n",
        "\n",
        "**Why?** Each retrieved text needs a compact rep (like sentence embedding) to fuse efficiently. Mini-block captures semantics without full encoder cost.\n",
        "\n",
        "**Coding Tips:**\n",
        "- Use `torch.tril` for causal mask—prevents info leak.\n",
        "- If mask provided, flatten it too (for padding in retrieved).\n",
        "\n",
        "\n",
        "#### 4. `aggregate_retrieved(self, z, retrieved_repr, retrieval_weights=None)`: Fusing z with Retrieved\n",
        "Here, `z` (latent [B, latent_size]) \"consults\" the k retrieved reps via attention. Output: Single fused vector [B, n_embd].\n",
        "\n",
        "**What it does (step-by-step):**\n",
        "1. **Project z:** Linear to [B, n_embd], unsqueeze to [B, 1, n_embd] (as \"query\").\n",
        "2. **Weights:** If none, uniform 1/k. Else, use DB softmax weights.\n",
        "3. **Weight reps:** Multiply `retrieved_repr` [B, k, n_embd] by weights [B, k] → weighted.\n",
        "4. **Concat:** z_query + weighted_reps → [B, k+1, n_embd] (query first).\n",
        "5. **Self-Attention:** `SimpleAttention` on this short seq (len=k+1=6). Causal mask (query sees all, but order preserved).\n",
        "6. **Extract:** Take first position (query output) → [B, n_embd] aggregated bias.\n",
        "\n",
        "\n",
        "\n",
        "**Why?** Attention lets z dynamically weigh retrieved info ( focus on most similar). Causal mask adds order (z first). Light: O(k^2) tiny for k=5.\n",
        "\n",
        "\n",
        "\n",
        "#### 5. `forward(self, input_ids, z, attention_mask=None, retrieved_ids=None, retrieval_weights=None)`: The Full Pass\n",
        "Ties it together: Baseline decode + optional retrieval bias.\n",
        "\n",
        "**What it does (step-by-step):**\n",
        "1. **Base Embed:** Tokens + pos + dropout → [B, T, n_embd].\n",
        "2. **Add z:** Project z [B, latent_size] → [B, 1, n_embd], broadcast + to all positions.\n",
        "3. **Retrieval Bias (if present):**\n",
        "   - Encode retrieved → aggregate → project → expand [B, 1, n_embd] → add to x.\n",
        "   - Else: Zero bias (baseline fallback).\n",
        "4. **Masks:** Causal (tril) * padding → full mask [B,1,1,T].\n",
        "5. **Main Blocks:** Stack n_layer causal transformer blocks.\n",
        "6. **Output:** LN(layer normalization) + LM head → logits [B, T, vocab_size].\n",
        "\n",
        "\n",
        "\n",
        "#### Putting It All Together: Coding Exercise\n",
        "1. **Start Simple:** Implement `__init__` + baseline forward (copy from `ConditionalDecoder`).\n",
        "2. **Add Retrieval:** Stub `encode_retrieved` (just mean-pool embeds, no block).\n",
        "3. **Fuse Lightly:** In forward, add a random bias if retrieved (test shapes).\n",
        "4. **Full Impl:** Add mini-block + attn; debug with `torch.manual_seed(42)`.\n",
        "5. **Integrate:** In `RegaVAE`, set `self.decoder = RetrievalAugmentedDecoder(config)` if `use_retrieval=True`.\n",
        "6. **Train/Test:**"
      ],
      "metadata": {
        "id": "ljIN-z9KusMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "m5XttS-l0GYg"
      },
      "outputs": [],
      "source": [
        "class RetrievalAugmentedDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_retrievals = 5 # k from DB search\n",
        "\n",
        "        # Baseline Decoder Parts\n",
        "        self.token_embed = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_embed = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.latent_proj = nn.Linear(config.latent_size, config.n_embd) # Project z to embedding dim\n",
        "\n",
        "        # Retrieval Augmentation Parts\n",
        "        # Mini-encoder for retrieved texts\n",
        "        self.retrieval_encoder_block = SimpleBlock(config) # Use one SimpleBlock\n",
        "        self.retrieval_ln = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.retrieval_pool = nn.AdaptiveAvgPool1d(1) # Pool over sequence length to get [B*k, 1, n_embd]\n",
        "\n",
        "        # Aggregation Attention (z as query, retrieved_repr as key/value)\n",
        "        self.aggregation_attn = SimpleAttention(config)\n",
        "        self.aggregation_ln = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.aggregation_proj = nn.Linear(config.n_embd, config.n_embd) # Project aggregated result\n",
        "\n",
        "        # Main Decoder Transformer Blocks\n",
        "        self.blocks = nn.ModuleList([SimpleBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Output logits\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Consistent init\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                nn.init.normal_(module.weight, std=0.02)\n",
        "\n",
        "    def encode_retrieved(self, retrieved_ids, retrieved_mask=None):\n",
        "        B, k, T = retrieved_ids.shape\n",
        "        retrieved_ids = retrieved_ids.view(B * k, T) # Flatten B and k\n",
        "        retrieved_mask = retrieved_mask.view(B * k, T) if retrieved_mask is not None else None\n",
        "\n",
        "        # Embed + Pos\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=retrieved_ids.device).unsqueeze(0)\n",
        "        token_embeddings = self.token_embed(retrieved_ids)\n",
        "        position_embeddings = self.pos_embed(pos)\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "\n",
        "        # Mini-Transformer Block\n",
        "        mask = None\n",
        "        if retrieved_mask is not None:\n",
        "             # Causal mask combined with padding mask for each retrieved item\n",
        "             causal_mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=retrieved_ids.device)).unsqueeze(0).unsqueeze(0) # [1, 1, T, T]\n",
        "             pad_mask = retrieved_mask.unsqueeze(1).unsqueeze(2) # [B*k, 1, 1, T]\n",
        "             mask = causal_mask * pad_mask # [B*k, 1, T, T]\n",
        "        else:\n",
        "             # Just causal mask if no padding mask\n",
        "             mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=retrieved_ids.device)).unsqueeze(0).unsqueeze(0).expand(B*k, -1, -1, -1)\n",
        "\n",
        "\n",
        "        x = self.retrieval_encoder_block(x, mask)\n",
        "\n",
        "        # LayerNorm\n",
        "        x = self.retrieval_ln(x)\n",
        "\n",
        "        # Pool (Mean over time dimension)\n",
        "        # Apply mask before pooling\n",
        "        if retrieved_mask is not None:\n",
        "             retrieved_mask_float = retrieved_mask.float().unsqueeze(-1) # [B*k, T, 1]\n",
        "             sum_x = torch.sum(x * retrieved_mask_float, dim=1) # [B*k, n_embd]\n",
        "             sum_mask = torch.sum(retrieved_mask_float, dim=1).clamp(min=1.0) # [B*k, 1]\n",
        "             retrieved_repr_flat = sum_x / sum_mask # [B*k, n_embd]\n",
        "        else:\n",
        "            retrieved_repr_flat = x.mean(dim=1) # [B*k, n_embd]\n",
        "\n",
        "\n",
        "        # Reshape back to [B, k, n_embd]\n",
        "        retrieved_repr = retrieved_repr_flat.view(B, k, self.config.n_embd)\n",
        "\n",
        "        return retrieved_repr\n",
        "\n",
        "\n",
        "    def aggregate_retrieved(self, z, retrieved_repr, retrieval_weights=None):\n",
        "        # z: [B, latent_size]\n",
        "        # retrieved_repr: [B, k, n_embd]\n",
        "        # retrieval_weights: [B, k]\n",
        "\n",
        "        # Project z to n_embd for attention\n",
        "        z_proj = self.latent_proj(z) # [B, n_embd]\n",
        "        z_query = z_proj.unsqueeze(1) # [B, 1, n_embd]\n",
        "\n",
        "        # Combine z_query and retrieved_repr for attention input\n",
        "        # Treat retrieved_repr as key and value\n",
        "        attn_input = torch.cat([z_query, retrieved_repr], dim=1) # [B, k+1, n_embd]\n",
        "\n",
        "        # Create attention mask for aggregation: z attends to everything, retrieved attend to themselves and z\n",
        "        # Causal mask for the k+1 sequence (z is at pos 0)\n",
        "        agg_T = k + 1\n",
        "        agg_causal_mask = torch.tril(torch.ones(agg_T, agg_T, dtype=torch.bool, device=z.device)).unsqueeze(0).unsqueeze(0) # [1, 1, k+1, k+1]\n",
        "\n",
        "\n",
        "        # Apply aggregation attention\n",
        "        aggregated_output = self.aggregation_attn(attn_input, agg_causal_mask) # [B, k+1, n_embd]\n",
        "\n",
        "        # Extract the aggregated result for z (first position)\n",
        "        aggregated_z = aggregated_output[:, 0, :] # [B, n_embd]\n",
        "\n",
        "        # Apply LayerNorm\n",
        "        aggregated_z = self.aggregation_ln(aggregated_z)\n",
        "\n",
        "        # Optional: Project the aggregated result\n",
        "        aggregated = self.aggregation_proj(aggregated_z) # [B, n_embd]\n",
        "\n",
        "\n",
        "        # Incorporate retrieval weights if provided (optional, can be done during fusion)\n",
        "        # Here, we'll assume the attention mechanism handles the weighting implicitly\n",
        "        # based on query-key similarity, but we could also use the provided weights\n",
        "        # to scale the `retrieved_repr` before the aggregation attention if desired.\n",
        "        # For simplicity here, we rely on the attention mechanism itself.\n",
        "\n",
        "        return aggregated # [B, n_embd]\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, z, attention_mask=None, retrieved_ids=None, retrieval_weights=None):\n",
        "        B, T = input_ids.shape\n",
        "\n",
        "        # Positions\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        # Token + position embeddings + dropout\n",
        "        token_embeddings = self.token_embed(input_ids)\n",
        "        position_embeddings = self.pos_embed(pos)\n",
        "        x = self.drop(token_embeddings + position_embeddings) # [B, T, n_embd]\n",
        "\n",
        "\n",
        "        # Add latent z\n",
        "        z_emb = self.latent_proj(z).unsqueeze(1) # [B, 1, n_embd]\n",
        "        x = x + z_emb # Add to all time steps [B, T, n_embd]\n",
        "\n",
        "        # Add Retrieval Bias (if present)\n",
        "        retrieval_bias = torch.zeros_like(x) # [B, T, n_embd]\n",
        "        if retrieved_ids is not None and retrieval_weights is not None and self.num_retrievals > 0:\n",
        "            # Encode retrieved texts\n",
        "            retrieved_repr = self.encode_retrieved(retrieved_ids) # [B, k, n_embd]\n",
        "\n",
        "            # Aggregate retrieved reps with z\n",
        "            aggregated_info = self.aggregate_retrieved(z, retrieved_repr, retrieval_weights) # [B, n_embd]\n",
        "\n",
        "            # Expand and add as bias to sequence (can be added to x or later layers)\n",
        "            # Adding as a bias to the initial embeddings\n",
        "            retrieval_bias = aggregated_info.unsqueeze(1) # [B, 1, n_embd]\n",
        "            x = x + retrieval_bias # Add to all time steps\n",
        "\n",
        "        # Masks\n",
        "        causal_mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=input_ids.device)).unsqueeze(0).unsqueeze(0) # [1, 1, T, T]\n",
        "        mask = causal_mask\n",
        "        if attention_mask is not None:\n",
        "            pad_mask = attention_mask.unsqueeze(1).unsqueeze(2) # [B, 1, 1, T]\n",
        "            mask = causal_mask * pad_mask # Combine causal and padding masks\n",
        "\n",
        "        # Transformer stack\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Output logits\n",
        "        logits = self.lm_head(x) # [B, T, vocab_size]\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "rybSOYZmREjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08400b53-3df4-46eb-edf5-1edadd0dc770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING REGAVAE WITH RETRIEVAL\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING REGAVAE WITH RETRIEVAL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "regavae_model = RegaVAE(\n",
        "    vocab_size=len(tokenizer),\n",
        "    embed_dim=256,\n",
        "    latent_dim=128,  # Larger latent space\n",
        "    use_retrieval=True\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('improved_checkpoints/best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(model)"
      ],
      "metadata": {
        "id": "XTwm2YoSlc5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81c9b34-5916-42c2-c1e5-0cf5ce523f6f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegaVAE(\n",
            "  (encoder): ConditionalEncoder(\n",
            "    (token_embed): Embedding(50257, 256)\n",
            "    (pos_embed): Embedding(1024, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0-1): 2 x SimpleBlock(\n",
            "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): SimpleAttention(\n",
            "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc_mu): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (fc_logvar): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (prior_net): PriorNetwork(\n",
            "    (token_embed): Embedding(50257, 256)\n",
            "    (pos_embed): Embedding(1024, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0-1): 2 x SimpleBlock(\n",
            "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): SimpleAttention(\n",
            "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc_mu): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (fc_logvar): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (decoder): ConditionalDecoder(\n",
            "    (token_embed): Embedding(50257, 256)\n",
            "    (pos_embed): Embedding(1024, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (latent_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (blocks): ModuleList(\n",
            "      (0-1): 2 x SimpleBlock(\n",
            "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): SimpleAttention(\n",
            "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "CxzrAmX7moe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48039459-bac2-4456-f2ad-062d77a9ca8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transferring encoder weights from baseline...\n",
            "Loading retrieval database...\n",
            "RegaVAE params: 58,274,048\n"
          ]
        }
      ],
      "source": [
        "print(\"Transferring encoder weights from baseline...\")\n",
        "regavae_model.encoder.load_state_dict(model.encoder.state_dict())\n",
        "regavae_model.prior_net.load_state_dict(model.prior_net.state_dict())\n",
        "\n",
        "# Set retrieval database\n",
        "print(\"Loading retrieval database...\")\n",
        "#retrieval_db = RetrievalDatabase(latent_dim=128, metric='cosine')  # Match dim\n",
        "#retrieval_db.load('retrieval_db')\n",
        "regavae_model.set_retrieval_db(retrieval_db)\n",
        "print(f\"RegaVAE params: {sum(p.numel() for p in regavae_model.parameters()):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regavae_train_losses, regavae_val_losses = train_regavae(\n",
        "    model=regavae_model,  # Your RegaVAE instance\n",
        "    train_loader=train_loader,\n",
        "    valid_loader=valid_loader,\n",
        "    num_epochs=22,  # Shorter for fine-tuning\n",
        "    learning_rate=5e-4,  # Lower to avoid overwriting baseline\n",
        "    cycle_iters=11,  # Gentle annealing\n",
        "    tokenizer=tokenizer,  # For tokenizing retrieved texts\n",
        "    save_path='regavae_checkpoints',  # Saves best_model.pth etc.\n",
        "    gradient_accumulation_steps=1,\n",
        "    patience=2,\n",
        "     # Gentle KL clamp\n",
        "    source_len=20,  # Yahoo query avg\n",
        "    use_dynamic_source=True,  # Adaptive to seq len\n",
        "    freeze_encoder=True  # Default: Yes, focus on decoder\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iu78Nu4OqqP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4984ae-77e2-473f-a00d-3714124dad5d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/22 (Train): 100%|██████████| 196/196 [01:25<00:00,  2.29it/s]\n",
            "Evaluating RegaVAE: 100%|██████████| 20/20 [00:07<00:00,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/22: Train Loss: 14.4849, Train Recon: 3.8127, Train KL: 29.0753\n",
            "                Valid Loss: 4.5372, Valid Recon: 4.3882, Valid KL: 0.1490\n",
            "Validation loss did not improve for 2 epochs.\n",
            "Early stopping after 2 epochs without improvement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "OtMLMCupt4NW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7796e952-ee6b-466f-90f9-e6edbd006ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best RegaVAE: Val Loss 4.5277\n",
            "RegaVAE(\n",
            "  (encoder): ConditionalEncoder(\n",
            "    (token_embed): Embedding(50257, 256)\n",
            "    (pos_embed): Embedding(1024, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0-1): 2 x SimpleBlock(\n",
            "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): SimpleAttention(\n",
            "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc_mu): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (fc_logvar): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (prior_net): PriorNetwork(\n",
            "    (token_embed): Embedding(50257, 256)\n",
            "    (pos_embed): Embedding(1024, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0-1): 2 x SimpleBlock(\n",
            "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): SimpleAttention(\n",
            "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc_mu): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (fc_logvar): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (decoder): RetrievalAugmentedDecoder(\n",
            "    (token_embed): Embedding(50257, 256)\n",
            "    (pos_embed): Embedding(1024, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (latent_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (retrieval_encoder_block): SimpleBlock(\n",
            "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): SimpleAttention(\n",
            "        (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "        (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (retrieval_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (retrieval_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "    (aggregation_attn): SimpleAttention(\n",
            "      (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "      (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (aggregation_ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (aggregation_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (blocks): ModuleList(\n",
            "      (0-1): 2 x SimpleBlock(\n",
            "        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): SimpleAttention(\n",
            "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load('regavae_checkpoints/best_model.pth')\n",
        "regavae_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "regavae_model.eval()  # Switch to eval\n",
        "print(f\"Loaded best RegaVAE: Val Loss {checkpoint['best_val_loss']:.4f}\")\n",
        "print(regavae_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "fn66UhBmA37o"
      },
      "outputs": [],
      "source": [
        "def generate_conditional_fixed(model, tokenizer, source_text, max_length=100,\n",
        "                               temperature=1.0, top_k=50, top_p=0.95,\n",
        "                               repetition_penalty=1.2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Updated for RegaVAE: Includes retrieval query with mu_prior for fusion.\n",
        "    Generates conditional text from source (prompt). FIXED: Unsqueeze next_token to 2D [1,1] for cat.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode source\n",
        "        encoded = tokenizer(source_text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
        "        source_ids = encoded['input_ids'].to(device)\n",
        "        source_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "        # Sample z from prior p(z|c)\n",
        "        mu_prior, logvar_prior = model.prior_net(source_ids, source_mask)\n",
        "        z = model.reparameterize(mu_prior, logvar_prior)  # Or z = mu_prior for deterministic\n",
        "\n",
        "        # RETRIEVAL: Query DB with mu_prior for gen (B=1)\n",
        "        retrieved_ids = None\n",
        "        retrieval_weights = None\n",
        "        if model.use_retrieval and model.retrieval_db is not None:\n",
        "            query_vectors = mu_prior.cpu().numpy()  # [1, latent_dim]\n",
        "            retrieved_texts, _, weights_np = model.retrieval_db.search(query_vectors, k=5)\n",
        "            retrieval_weights = torch.tensor(weights_np, dtype=torch.float).to(device)\n",
        "\n",
        "            # Tokenize (B=1)\n",
        "            encoded_list = []\n",
        "            for t in retrieved_texts[0]:  # Single list of 5 texts (B=1)\n",
        "                encoded = tokenizer(t, max_length=64, padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
        "                encoded_list.append(encoded.squeeze(0))  # squeeze(0) → [64] 1D\n",
        "            retrieved_ids = torch.stack(encoded_list).unsqueeze(0).to(device)  # [1, 5, 64] 3D\n",
        "\n",
        "        # Start generation (append to source)\n",
        "        generated = source_ids.clone()  # [1, source_len]\n",
        "        source_len = source_ids.size(1)\n",
        "\n",
        "        for step in range(max_length):\n",
        "            # Forward with retrieval\n",
        "            curr_mask = torch.ones(1, generated.size(1), device=device)\n",
        "            logits = model.decoder(generated, z, curr_mask, retrieved_ids=retrieved_ids, retrieval_weights=retrieval_weights)\n",
        "\n",
        "            # Next token logits\n",
        "            next_logits = logits[0, -1, :].clone() / temperature\n",
        "\n",
        "            # Repetition penalty (same as baseline)\n",
        "            if repetition_penalty != 1.0:\n",
        "                for token_id in set(generated[0].tolist()):\n",
        "                    if next_logits[token_id] < 0:\n",
        "                        next_logits[token_id] *= repetition_penalty\n",
        "                    else:\n",
        "                        next_logits[token_id] /= repetition_penalty\n",
        "\n",
        "            # Top-k (same)\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = next_logits < torch.topk(next_logits, min(top_k, next_logits.size(-1)))[0][..., -1, None]\n",
        "                next_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Top-p (same, 1D fixed)\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)\n",
        "                cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cum_probs > top_p\n",
        "                sorted_indices_to_remove[0] = False\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
        "                next_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample & FIXED: Unsqueeze to [1,1] for cat with generated [1, len]\n",
        "            probs = F.softmax(next_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).unsqueeze(0)  # FIXED: unsqueeze(0) → [1,1]\n",
        "            generated = torch.cat([generated, next_token], dim=1)  # Now 2D + 2D\n",
        "\n",
        "            # Stops\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "            if generated.size(1) >= source_len + max_length:\n",
        "                break\n",
        "\n",
        "        # Decode generated part only\n",
        "        generated_tokens = generated[0, source_len:].cpu().tolist()\n",
        "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Why is the sky blue?\"\n",
        "generated = generate_conditional_fixed(regavae_model, tokenizer, prompt, max_length=20, temperature=0.9)\n",
        "print(f\"Prompt: {prompt}\\nGenerated: {generated}\")"
      ],
      "metadata": {
        "id": "rS3lbM8prh25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b4774f-c3df-46c3-cf3b-03cfbbc2717c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Why is the sky blue?\n",
            "Generated:  and _UNK in the air ? i ca n't get rid of my _UNK ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EVALUATION ON TEST SET FOR BOTH MODELS"
      ],
      "metadata": {
        "id": "csdglX39XtB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally 10 epoch of training for both the model should generate these numbers (around)"
      ],
      "metadata": {
        "id": "1MLfXX9Qaj9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2025-10-20 111716.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "flk2DPzcadJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_baseline = torch.load('improved_checkpoints/best_model.pth')\n",
        "model.load_state_dict(checkpoint_baseline['model_state_dict'])\n",
        "model.eval()\n",
        "print(\"Loaded best baseline model.\")\n",
        "\n",
        "checkpoint_regavae = torch.load('regavae_checkpoints/best_model.pth')\n",
        "regavae_model.load_state_dict(checkpoint_regavae['model_state_dict'])\n",
        "regavae_model.eval()\n",
        "print(\"Loaded best RegaVAE model.\")\n",
        "\n",
        "\n",
        "def generate_from_test(model, test_loader, tokenizer, num_samples=200, source_len=20, gen_len=30,\n",
        "                       temperature=0.8, top_k=40, top_p=0.9, repetition_penalty=1.1, device='cuda'):\n",
        "\n",
        "    model.eval()\n",
        "    generated_texts = []\n",
        "    test_iter = iter(test_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_samples):\n",
        "            try:\n",
        "                batch = next(test_iter)\n",
        "            except StopIteration:\n",
        "                test_iter = iter(test_loader)\n",
        "                batch = next(test_iter)\n",
        "\n",
        "\n",
        "            input_ids = batch['input_ids'][0:1].to(device)  # [1, max_length]\n",
        "            attention_mask = batch['attention_mask'][0:1].to(device)\n",
        "            original_text = batch['text'][0]  # Full original for reference\n",
        "\n",
        "\n",
        "            source_ids = input_ids[:, :source_len]\n",
        "            source_mask = attention_mask[:, :source_len]\n",
        "\n",
        "\n",
        "            mu_prior, logvar_prior = model.prior_net(source_ids, source_mask)\n",
        "            z = model.reparameterize(mu_prior, logvar_prior)\n",
        "\n",
        "\n",
        "\n",
        "            retrieved_ids = None\n",
        "            retrieval_weights = None\n",
        "            if hasattr(model, 'use_retrieval') and model.use_retrieval and model.retrieval_db is not None:\n",
        "                query_vectors = mu_prior.cpu().numpy()\n",
        "                retrieved_texts, _, weights_np = model.retrieval_db.search(query_vectors, k=5)\n",
        "                retrieval_weights = torch.tensor(weights_np).to(device)\n",
        "\n",
        "                encoded_list = [tokenizer(t, max_length=64, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].squeeze(0)\n",
        "                                for t in retrieved_texts[0]]\n",
        "                retrieved_ids = torch.stack(encoded_list).unsqueeze(0).to(device)  # [1,5,64]\n",
        "\n",
        "\n",
        "            generated = source_ids.clone()\n",
        "            for step in range(gen_len):\n",
        "                curr_mask = torch.ones(1, generated.size(1), device=device)\n",
        "                logits = model.decoder(generated, z, curr_mask, retrieved_ids, retrieval_weights) if retrieved_ids is not None else \\\n",
        "                         model.decoder(generated, z, curr_mask)\n",
        "\n",
        "                next_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "\n",
        "                if repetition_penalty > 1.0:\n",
        "                    for token_id in set(generated[0].tolist()):\n",
        "                        next_logits[token_id] /= repetition_penalty\n",
        "\n",
        "\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = next_logits < torch.topk(next_logits, top_k)[0][-1, None]\n",
        "                    next_logits[indices_to_remove] = float('-inf')\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)\n",
        "                    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                    sorted_indices_to_remove = cum_probs > top_p\n",
        "                    sorted_indices_to_remove[0] = False\n",
        "                    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
        "                    next_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "                probs = F.softmax(next_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
        "                generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "                if next_token.item() == tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "\n",
        "            gen_tokens = generated[0, source_len:].cpu().tolist()\n",
        "            gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "            generated_texts.append(gen_text)\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "\n",
        "baseline_generated = generate_from_test(\n",
        "    model, test_loader, tokenizer, num_samples=200, source_len=15, gen_len=20,\n",
        "    temperature=0.8, top_k=40, top_p=0.9, repetition_penalty=1.1, device=device\n",
        ")\n",
        "\n",
        "\n",
        "regavae_generated = generate_from_test(\n",
        "    regavae_model, test_loader, tokenizer, num_samples=200, source_len=15, gen_len=20,\n",
        "    temperature=0.8, top_k=40, top_p=0.9, repetition_penalty=1.1, device=device\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nComputing full metrics for baseline...\")\n",
        "baseline_metrics = evaluate_all_metrics(\n",
        "    model, test_loader, baseline_generated,\n",
        "    use_retrieval=False, tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"\\nComputing full metrics for RegaVAE...\")\n",
        "regavae_metrics = evaluate_all_metrics(\n",
        "    regavae_model, test_loader, regavae_generated,\n",
        "    use_retrieval=True, tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"\\nComparing models...\")\n",
        "comparison = compare_models(baseline_metrics, regavae_metrics)\n",
        "\n"
      ],
      "metadata": {
        "id": "OTv2krNhYaN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60bf871-b9df-4d15-9a45-4e2d3652b83c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best baseline model.\n",
            "Loaded best RegaVAE model.\n",
            "\n",
            "Computing full metrics for baseline...\n",
            "Computing evaluation metrics...\n",
            "1. Computing Perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing PPL: 100%|██████████| 20/20 [00:01<00:00, 18.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: Baseline | Final PPL: 145.86\n",
            "   PPL: 145.86\n",
            "2. Computing Self-BLEU...\n",
            "   Self-BLEU: 0.3645\n",
            "3. Computing Distinct-n metrics...\n",
            "   Distinct-1: 0.0876\n",
            "   Distinct-2: 0.3932\n",
            "   Distinct-3: 0.7376\n",
            "\n",
            "Computing full metrics for RegaVAE...\n",
            "Computing evaluation metrics...\n",
            "1. Computing Perplexity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing PPL: 100%|██████████| 20/20 [00:07<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode: Retrieval | Final PPL: 66.72\n",
            "   PPL: 66.72\n",
            "2. Computing Self-BLEU...\n",
            "   Self-BLEU: 0.3002\n",
            "3. Computing Distinct-n metrics...\n",
            "   Distinct-1: 0.1364\n",
            "   Distinct-2: 0.5140\n",
            "   Distinct-3: 0.8011\n",
            "\n",
            "Comparing models...\n",
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON\n",
            "======================================================================\n",
            "\n",
            "Metric          Baseline     RegaVAE      Change       Direction\n",
            "----------------------------------------------------------------------\n",
            "PPL             145.8580     66.7238          +54.25% ↓\n",
            "Self-BLEU       0.3645       0.3002           +17.64% ↓\n",
            "distinct_1      0.0876       0.1364           +55.80% ↑\n",
            "distinct_2      0.3932       0.5140           +30.71% ↑\n",
            "distinct_3      0.7376       0.8011            +8.61% ↑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxrAkMKGj0ab"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}